{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import pickle\n",
    "from line_profiler import LineProfiler\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score, confusion_matrix, recall_score, f1_score\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.utils import shuffle\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Physicochemical Properties\n",
    "\n",
    "Most sources can be found on AAindex\n",
    "\n",
    "PKA source: D.R. Lide, Handbook of Chemistry and Physics, 72nd Edition, CRC Press, Boca Raton, FL, 1991. (Sigma Aldrich website)\n",
    "\n",
    "EIIP: Electron-ion interaction potential (Veljkovic et al., 1985)\n",
    "\n",
    "LEP: No citation, sorta implicit (NOT VERIFIED!)\n",
    "\n",
    "Wiener Index: ?\n",
    "\n",
    "Molecular Mass: Wikipedia, implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMINO_ACID_INDICES = {'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'Q': 5, 'E': 6, 'G': 7, 'H': 8, 'I': 9, \n",
    "                      'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19}\n",
    "\n",
    "PKA_AMINO_GROUP = np.array([9.69, 9.04, 8.80, 9.60, 10.28, 9.13, 9.67, 9.60, 9.17, 9.60,\n",
    "                            9.60, 8.95, 9.21, 9.13, 10.60, 9.15, 9.10, 9.39, 9.11, 9.62])\n",
    "PKA_CARBOXYL_GROUP = np.array([2.34, 2.17, 2.02, 1.88, 1.96, 2.17, 2.19, 2.34, 1.82, 2.36,\n",
    "                               2.36, 2.18, 2.28, 1.83, 1.99, 2.21, 2.09, 2.83, 2.20, 2.32])\n",
    "EIIP = np.array([0.03731, 0.09593, 0.00359, 0.12630, 0.08292, 0.07606, 0.00580, 0.00499, 0.02415, 0.0000, \n",
    "                 0.0000, 0.03710, 0.08226, 0.09460, 0.01979, 0.08292, 0.09408, 0.05481, 0.05159, 0.00569])\n",
    "LONE_ELECTRON_PAIRS = np.array([0, 0, 1, 2, 1, 1, 2, 0, 1, 0, \n",
    "                                0, 0, 0, 0, 0, 1, 1, 0, 1, 0])\n",
    "WIENER_INDEX = np.array([0.3466, 0.1156, 0.3856, 0.2274, 0.0501, 0.6379, 0.1938, 0.1038, 0.2013,\n",
    "                       0.2863, 0.1071, 0.7767, 0.7052, 0.3419, 0.0957, 0.4375, 0.9320, 0.1000, 0.1969, 0.9000])\n",
    "MOLECULAR_MASS = np.array([89.094, 174.203, 132.119, 133.104, 121.154, 146.146, 147.131, 75.067, 155.156, 131.175,\n",
    "                           131.175, 146.189, 149.208, 165.192, 115.132, 105.093, 119.119, 204.228, 181.191, 117.148])\n",
    "\n",
    "# pKa_amino_group = np.array([9.87, 8.99, 8.72, 9.90, 10.70, 9.13, 9.47, 9.78,\n",
    "#                            9.33, 9.76, 9.74, 9.06, 9.28, 9.31, 10.64, 9.21, 9.10, 9.41, 9.21, 9.74])\n",
    "# pKa_carboxyl_group = np.array([2.35, 1.82, 2.14, 1.99, 1.92, 2.17, 2.10, 2.35,\n",
    "#                               1.80, 2.32, 2.33, 2.16, 2.13, 2.20, 1.95, 2.19, 2.09, 2.46, 2.20, 2.29])\n",
    "# eiip = np.array([0.0373, 0.0959, 0.0036, 0.1263, 0.0829, 0.0761, 0.0057, 0.0050, 0.0242,\n",
    "#                 0.0000, 0.0000, 0.0371, 0.0823, 0.0946, 0.0198, 0.0829, 0.0941, 0.0548, 0.0516, 0.0058])\n",
    "# lone_electron_pairs = np.array(\n",
    "#     [0, 0, 1, 2, 1, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0])\n",
    "# winer_index = np.array([0.3466, 0.1156, 0.3856, 0.2274, 0.0501, 0.6379, 0.1938, 0.1038, 0.2013,\n",
    "#                        0.2863, 0.1071, 0.7767, 0.7052, 0.3419, 0.0957, 0.4375, 0.9320, 0.1000, 0.1969, 0.9000])\n",
    "# molecular_mass = np.array([71.078, 156.186, 114.103, 115.087, 103.143, 128.129, 129.114, 57.051, 137.139,\n",
    "#                           113.158, 113.158, 128.172, 131.196, 147.174, 97.115, 87.077, 101.104, 186.210, 163.173, 99.131])\n",
    "\n",
    "PP_LIST = [PKA_AMINO_GROUP, PKA_CARBOXYL_GROUP, EIIP, LONE_ELECTRON_PAIRS, WIENER_INDEX, MOLECULAR_MASS]\n",
    "# PP_LIST = [pKa_amino_group, pKa_carboxyl_group, eiip, lone_electron_pairs, winer_index, molecular_mass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amino Acid Composition (AAC) groups - Polarity Charge\n",
    "# C1; C2; C3; C4 \n",
    "# (polar amino acid with positive charge, polar amino acid with negative charge, noncharged\n",
    "# polar amino acid, nonpolar amino acid).\n",
    "\n",
    "AAC_C1 = ['G', 'A', 'V', 'L', 'I', 'F', 'W', 'M', 'P']\n",
    "AAC_C2 = ['S', 'T', 'C', 'Y', 'N', 'Q']\n",
    "AAC_C3 = ['D', 'E']\n",
    "AAC_C4 = ['R', 'K', 'H']\n",
    "\n",
    "AAC_C_LIST = [AAC_C1, AAC_C2, AAC_C3, AAC_C4]\n",
    "\n",
    "# Amino Acid Composition (AAC) groups - Hydrohpobicity\n",
    "# H1;H2;H3;H4  (strong hydrophobic residue, weak hydrophobic residue, strong hydrophilic residue, weak hydrophilic residue).\n",
    "# This scale is obtained from Kyte and Doolittle (1982). \n",
    "# K&D scale from 0 to +-2.0 is considered weak, >2.0 is strong hydrophobicity, and <-2.0 is strong hydrophilic. \n",
    "\n",
    "\n",
    "AAC_H1 = ['I', 'V', 'L', 'F', 'C']\n",
    "AAC_H2 = ['M', 'A']\n",
    "AAC_H3 = ['H', 'Q', 'N', 'E', 'D', 'K', 'R']\n",
    "AAC_H4 = ['G', 'T', 'S', 'W', 'Y', 'P']\n",
    "\n",
    "AAC_H_LIST = [AAC_H1, AAC_H2, AAC_H3, AAC_H4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PP Matrix stored as a constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows: normalized pp properties \n",
    "# columns: amino acids\n",
    "def create_pp_matrix() -> np.ndarray:\n",
    "    pp_matrix = np.empty((len(PP_LIST), len(AMINO_ACID_INDICES)), dtype=float)\n",
    "    for i, pp in enumerate(PP_LIST):\n",
    "        max_val = np.max(pp)\n",
    "        min_val = np.min(pp)\n",
    "        pp_matrix[i] = (pp - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return pp_matrix\n",
    "\n",
    "# Constant PP_MATRIX\n",
    "PP_MATRIX = create_pp_matrix()\n",
    "# print(PP_MATRIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBV\n",
    "\n",
    "Source: Shen, Juwen, et al. \"Predicting proteinâ€“protein interactions based only on sequences information.\" Proceedings of the National Academy of Sciences 104.11 (2007): 4337-4341. (Supp. information)\n",
    "\n",
    "Note: We use 7 classes here instead of 6. It was not mentioned why they used 6 classes only, when the source mentioned that amino acids are grouped into 7 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "obv_classes = {\n",
    "    'A' : 0, 'G' : 0, 'V' : 0,\n",
    "    'I': 1, 'L': 1, 'F': 1, 'P': 1,\n",
    "    'Y': 2, 'M': 2, 'T': 2, 'S': 2,\n",
    "    'H': 3, 'N': 3, 'Q': 3, 'W': 3,\n",
    "    'R': 4, 'K': 4,\n",
    "    'D': 5, 'E': 5,\n",
    "    'C': 6\n",
    "}\n",
    "\n",
    "def generate_obv(amino_acid):\n",
    "    temp = np.zeros(7)\n",
    "    temp[obv_classes.get(amino_acid)] = 1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Window Instance from sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a string, then \n",
    "# extract list of instances by sliding a window through the sequence\n",
    "def get_instances_from_seq(seq : str, window_size : int = 9) -> list :\n",
    "    instances = list()\n",
    "    for i in range(len(seq) - window_size + 1):\n",
    "        instances.append(seq[i:i+window_size])\n",
    "    return instances\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate PSSM-PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate PSSM using psiblast from a given sequence.\"\"\"\n",
    "def generate_pssm(input_seq: str, num_iterations = 3) -> np.ndarray:\n",
    "    DB_PATH = \"./databases/uniprot_sprot.fasta\"\n",
    "    output_pssm = \"output.pssm\"\n",
    "\n",
    "    # Creating a temporary fasta file for input\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".fasta\") as temp_fasta:\n",
    "        SeqIO.write([SeqRecord(Seq(input_seq))], temp_fasta, \"fasta\")\n",
    "        temp_fasta_path = temp_fasta.name\n",
    "\n",
    "    # Running psiblast\n",
    "    try:\n",
    "        subprocess.run([\"psiblast\", \"-query\", temp_fasta_path, \"-db\", DB_PATH, \n",
    "                        \"-out_ascii_pssm\", output_pssm, \"-num_iterations\", str(num_iterations), \"-evalue\", \"0.001\"], \n",
    "                        check=True, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "    finally:\n",
    "        os.remove(temp_fasta_path)\n",
    "\n",
    "    # Reading PSSM output\n",
    "    try:\n",
    "        pssm_df = pd.read_csv(output_pssm, delim_whitespace=True, skiprows=3, header=None)\n",
    "        os.remove(output_pssm)  # Clean up PSSM file after reading\n",
    "        pssm_array = pssm_df.iloc[:-5, 2:22].to_numpy(dtype=int)\n",
    "        return pssm_array\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: PSSM file not found. Input Sequence: {input_seq}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "# Rescale pssm using sigmoid\n",
    "def rescale_pssm(input_pssm) -> np.ndarray:\n",
    "    input_pssm = 1/(1 + np.exp(-input_pssm))\n",
    "    return input_pssm\n",
    "\n",
    "# Get only the pssm rows that are relevant to the sequence\n",
    "def get_sliced_pssm(original_pssm : np.ndarray, start_index : int, window_size : int = 9):\n",
    "    return original_pssm[start_index : start_index + window_size, :]\n",
    "\n",
    "# generate_pssm(\"KPKNKDKDKKVPEPDNKKKKPKKEEEQKWKWWEEERYPEGIKWKFLEHKGPVFAPPYEPLPENVKFYYDGKVMKLSPKAEEVATFFAKMLDHEYTTKEIFRKNFFKDWRKEMTNEEKNIITNLSKCDFTQMSQYFKAQTEARKQMSKEEKLKIKEENEKLLKEYGFCIMDNHKERIANFKIEPPGLFRGRGNHPKMGMLKRRIMPEDIIINCSKDAKVPSPPPGHKWKEVRHDNKVTWLVSWTENIQGSIKYIMLNPSSRIKGEKDWQKYETARRLKKCVDKIRNQYREDWKSKEMKVRQRAVALYFIDKLALRAGNEKEEGETADTVGCCSLRVEHINLHPELDGQEYVVEFDFLGKDSIRYYNKVPVEKRVFKNLQLFMENKQPEDDLFDRLNTGILNKHLQDLMEGLTAKVFRTYNASITLQQQLKELTAPDENIPAKILSYNRANRAVAILCNHQRAPPKTFEKSMMNLQTKIDAKKEQLADARRDLKSAKADAKVMKDAKTKKVVESKKKAVQRLEEQLMKLEVQATDREENKQIALGTSKLNFLDPRITVAWCKKWGVPIEKIYNKTQREKFAWAIDMADEDYE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pssm_pp(pssm_matrix : np.ndarray, pp_matrix : np.ndarray) -> np.ndarray:\n",
    "    return pp_matrix @ pssm_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amino Acid Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AAC_PC takes in a sequence of 9 amino acids then outputs a list of 4 values\n",
    "def calculate_AAC_PC(seq : str):\n",
    "    window_size = len(seq)\n",
    "    \n",
    "    def get_c_i():\n",
    "        c_i = np.zeros((4, window_size - 1), dtype=int)\n",
    "        for gap in range(1, window_size):\n",
    "            for j in range(window_size - gap):\n",
    "                for index, aac_class in enumerate(AAC_C_LIST):\n",
    "                    if seq[j] in aac_class and seq[j + gap] in aac_class:\n",
    "                        c_i[index][gap - 1] += 1\n",
    "        # print(c_i)\n",
    "        return c_i\n",
    "    \n",
    "    def get_n_i():\n",
    "        n_i = [np.sum(seq.count(a) for a in aac_class) for aac_class in AAC_C_LIST]\n",
    "        # print(n_i)\n",
    "        return np.array(n_i)\n",
    "    \n",
    "    c_i = get_c_i()\n",
    "    n_i = get_n_i()\n",
    "    \n",
    "    output_aac_list = list()\n",
    "    for i in range(0, 4):\n",
    "        sum = 0\n",
    "        for k in range(0, window_size - 1):\n",
    "            first_term = ((c_i[i][k] / (window_size - k)) - (n_i[i]**2 / window_size**2))\n",
    "            if np.isnan(first_term):\n",
    "                first_term = 0\n",
    "            second_term = np.square(first_term) / (2 * (n_i[i]**2 / window_size**2))\n",
    "            if np.isnan(second_term):\n",
    "                second_term = 0\n",
    "            sum += (first_term + second_term)\n",
    "        output_aac_list.append(sum)\n",
    "    \n",
    "    # print(output_aac_list)\n",
    "    return output_aac_list\n",
    "\n",
    "def calculate_AAC_H(seq : str):\n",
    "    window_size = len(seq)\n",
    "    def get_h_i():\n",
    "        h_i = np.zeros((4, window_size - 1), dtype=int)\n",
    "        for gap in range(1, window_size):\n",
    "            for j in range(window_size - gap):\n",
    "                for index, aac_class in enumerate(AAC_H_LIST):\n",
    "                    if seq[j] in aac_class and seq[j + gap] in aac_class:\n",
    "                        h_i[index][gap - 1] += 1\n",
    "        # print(h_i)\n",
    "        return h_i\n",
    "    \n",
    "    def get_m_i():\n",
    "        m_i = [np.sum(seq.count(a) for a in aac_class) for aac_class in AAC_H_LIST]\n",
    "        # print(m_i)\n",
    "        return np.array(m_i)\n",
    "    \n",
    "    h_i = get_h_i()\n",
    "    m_i = get_m_i()\n",
    "    \n",
    "    output_aac_list = list()\n",
    "    for i in range(0, 4):\n",
    "        sum = 0\n",
    "        for k in range(0, window_size - 1):\n",
    "            first_term = ((h_i[i][k] / (window_size - k)) - (m_i[i]**2 / window_size**2))\n",
    "            first_term = 0 if np.isnan(first_term) else first_term\n",
    "            second_term = np.square(first_term) / (2 * (m_i[i]**2 / window_size**2))\n",
    "            second_term = 0 if np.isnan(second_term) else second_term\n",
    "            sum += (first_term + second_term)\n",
    "        output_aac_list.append(sum)\n",
    "    \n",
    "    # print(output_aac_list)\n",
    "    return output_aac_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_obv(seq: str):\n",
    "    full_obv = np.zeros((len(seq), 7)) \n",
    "    for idx, aa in enumerate(seq):\n",
    "        full_obv[idx] = generate_obv(aa)  \n",
    "    return full_obv.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-generate PSSMs and store to a numpy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pssms for a list of sequences, then save them to a pickle file for future use\n",
    "def pre_generate_pssm(input_df, file_name:str):\n",
    "    pssm_list = list()\n",
    "    for seq in input_df['seq']:\n",
    "        pssm = generate_pssm(seq)\n",
    "        pssm_list.append(pssm)\n",
    "    \n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(pssm_list, f)\n",
    "    return pssm_list\n",
    "    \n",
    "# list_of_train_pssms = pre_generate_pssm(pd.read_csv(\"./DRNA_TRAIN.csv\"), \"generated_pssms_train.pkl\")\n",
    "# print(len(list_of_train_pssms))\n",
    "# list_of_test_pssms = pre_generate_pssm(pd.read_csv(\"./DRNA_TEST.csv\"), \"generated_pssms_test.pkl\")\n",
    "# print(len(list_of_test_pssms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming these functions are correctly implemented\n",
    "def get_all_features_for_one_sequence(full_seq: str, dna_label: str, input_pssm : np.ndarray, seq_diso_values, domain_values, input_hhm : np.ndarray, window_size: int = 9) -> list:\n",
    "    seq_list = get_instances_from_seq(full_seq, window_size=window_size)  # Assuming this returns a list of sequences of length window_size\n",
    "    pssm = rescale_pssm(input_pssm)  # Assuming this returns a PSSM for the full_seq\n",
    "    # pssm = np.array(input_pssm)[:, :20]\n",
    "    # print(pssm.shape)\n",
    "\n",
    "    all_features_list = []  # Use a list to maintain structure\n",
    "\n",
    "    for index, seq in enumerate(seq_list):\n",
    "        # print(f\"Processing sequence {index} of {len(seq_list)}\")\n",
    "        current_residue_label = dna_label[index + window_size // 2]\n",
    "        if current_residue_label == '2':\n",
    "            # print(f\"Residue unknown at index {index}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        pssm_pp_features = create_pssm_pp(get_sliced_pssm(pssm, index, window_size).T, PP_MATRIX).flatten()\n",
    "        aac_features = np.append(calculate_AAC_PC(seq), calculate_AAC_H(seq))\n",
    "        obv_features = get_full_obv(seq) \n",
    "        diso_features = [0 if x < 0.5 else 1 for x in seq_diso_values[index:index+window_size]]\n",
    "        domain_features = domain_values[index:index+window_size]\n",
    "        # hhm_features = np.array(input_hhm[index:index+window_size]).flatten()\n",
    "        \n",
    "        # all_features = np.concatenate([pssm_pp_features, aac_features, obv_features, domain_features])\n",
    "        all_features = np.concatenate([pssm_pp_features, aac_features, obv_features, diso_features, domain_features])\n",
    "        all_features_list.append((all_features, current_residue_label))\n",
    "\n",
    "    return all_features_list\n",
    "\n",
    "# Generate feature vectors for each sequence in the training dataset\n",
    "def get_all_features_for_dataset(dataset: pd.DataFrame, generated_pssm_file, generated_diso_file, generated_domain_file, generated_hhm_file, window_size : int = 9) -> list:\n",
    "    full_pssm = list(pickle.load(open(generated_pssm_file, 'rb'))) \n",
    "    full_diso_values = list(pickle.load(open(generated_diso_file, 'rb')))   \n",
    "    full_domain_values = list(pickle.load(open(generated_domain_file, 'rb')))  \n",
    "    full_hhm_values = list(pickle.load(open(generated_hhm_file, 'rb')))\n",
    "    \n",
    "    all_features_list = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        if full_pssm[index] is None:\n",
    "            print(f\"Skipping sequence at index {index} due to missing PSSM\")\n",
    "            continue\n",
    "        if full_diso_values[index] is None:\n",
    "            print(f\"Diso value not available! Index: {index}\")\n",
    "            continue\n",
    "        # if full_domain_values[index] is None:\n",
    "        #     print(f\"Domain value not available! Index: {index}\")\n",
    "        #     continue\n",
    "        try:\n",
    "            all_features_list.extend(get_all_features_for_one_sequence(full_seq=row['seq'], \n",
    "                                                                       dna_label=row['dna_label'],\n",
    "                                                                       input_pssm = full_pssm[index], \n",
    "                                                                       seq_diso_values=full_diso_values[index],\n",
    "                                                                       domain_values=full_domain_values[index],\n",
    "                                                                       input_hhm=full_hhm_values[index],\n",
    "                                                                       window_size=window_size))\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error processing sequence at index {index}: {e}\")\n",
    "            continue\n",
    "    return all_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping sequence at index 119 due to missing PSSM\n",
      "Skipping sequence at index 179 due to missing PSSM\n",
      "Skipping sequence at index 211 due to missing PSSM\n",
      "Skipping sequence at index 234 due to missing PSSM\n",
      "Skipping sequence at index 264 due to missing PSSM\n",
      "Skipping sequence at index 268 due to missing PSSM\n",
      "Skipping sequence at index 293 due to missing PSSM\n",
      "Skipping sequence at index 349 due to missing PSSM\n",
      "Skipping sequence at index 357 due to missing PSSM\n",
      "Skipping sequence at index 368 due to missing PSSM\n",
      "Skipping sequence at index 387 due to missing PSSM\n",
      "Skipping sequence at index 391 due to missing PSSM\n",
      "Skipping sequence at index 403 due to missing PSSM\n",
      "Skipping sequence at index 405 due to missing PSSM\n",
      "Skipping sequence at index 410 due to missing PSSM\n",
      "Skipping sequence at index 422 due to missing PSSM\n",
      "Skipping sequence at index 436 due to missing PSSM\n",
      "Skipping sequence at index 439 due to missing PSSM\n",
      "Skipping sequence at index 452 due to missing PSSM\n",
      "Skipping sequence at index 5 due to missing PSSM\n",
      "Skipping sequence at index 16 due to missing PSSM\n",
      "Skipping sequence at index 51 due to missing PSSM\n",
      "Skipping sequence at index 58 due to missing PSSM\n",
      "Skipping sequence at index 66 due to missing PSSM\n",
      "X_train shape: (96548, 143)\n",
      "y_train shape: (96548,)\n",
      "X_test shape: (17952, 143)\n",
      "y_test shape: (17952,)\n"
     ]
    }
   ],
   "source": [
    "# Assuming training_dataset is loaded correctly\n",
    "training_dataset = pd.read_csv(\"DRNA_TRAIN.csv\")\n",
    "test_dataset = pd.read_csv(\"DRNA_TEST.csv\")\n",
    "\n",
    "all_training_features = get_all_features_for_dataset(training_dataset, \"generated_pssms_train.pkl\", \"disorder_preds_train.pkl\", \"dom_annotations_train.pkl\", \"generated_hhm_train.pkl\")\n",
    "all_test_features = get_all_features_for_dataset(test_dataset, \"generated_pssms_test.pkl\", \"disorder_preds_test.pkl\", \"dom_annotations_test.pkl\", \"generated_hhm_test.pkl\")\n",
    "\n",
    "# Separate into X_train and y_train\n",
    "X_train = [features for features, label in all_training_features]\n",
    "y_train = [label for features, label in all_training_features]\n",
    "X_test = [features for features, label in all_test_features]\n",
    "y_test = [label for features, label in all_test_features]\n",
    "\n",
    "# Optionally convert to numpy arrays for compatibility with scikit-learn\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Compilation of results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: purple' if v else '' for v in is_max]\n",
    "\n",
    "# # Train the model\n",
    "# rf_model = RandomForestClassifier(random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Get prediction probabilities\n",
    "# predictions = rf_model.predict_proba(X_test)\n",
    "# results = []\n",
    "\n",
    "# # Iterate over thresholds from 0.1 to 0.96 with a step of 0.02\n",
    "# for threshold in np.arange(0.1, 0.96, 0.02):\n",
    "#     y_pred = ['1' if p[1] >= threshold else '0' for p in predictions]\n",
    "    \n",
    "#     mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    \n",
    "#     tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=['0', '1']).ravel()\n",
    "    \n",
    "#     sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "#     specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    \n",
    "#     results.append((threshold, mcc, sensitivity, specificity))\n",
    "\n",
    "\n",
    "# results_df = pd.DataFrame(results, columns=['Threshold', 'MCC', 'Sensitivity', 'Specificity'])\n",
    "# results_df = results_df.round(2)\n",
    "# styled_results_df = results_df.style.format(\"{:.2f}\").apply(highlight_max, subset=['MCC'])\n",
    "# display(styled_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14222, 143) (14222,)\n"
     ]
    }
   ],
   "source": [
    "# Subsample X_train and y_train such that they contain equal amounts of positive and negative samples\n",
    "# Assuming y_train contains binary labels where 1 is positive and 0 is negative\n",
    "positive_indices = np.where(y_train_nn == '1')[0]\n",
    "negative_indices = np.where(y_train_nn == '0')[0]\n",
    "\n",
    "\n",
    "# # Determine the number of samples to subsample based on the smaller class\n",
    "n_samples = min(positive_indices.shape[0], negative_indices.shape[0]) \n",
    "\n",
    "# Randomly select n_samples from both positive and negative indices\n",
    "positive_subsample_indices = np.random.choice(positive_indices, n_samples, replace=False)\n",
    "negative_subsample_indices = np.random.choice(negative_indices, n_samples, replace=False)\n",
    "# unknkown_subsample_indices = np.random.choice(unknown_indices, n_samples, replace=True)\n",
    "\n",
    "# Concatenate the subsampled indices and then use them to create subsampled X_train and y_train\n",
    "subsample_indices = np.concatenate([positive_subsample_indices, negative_subsample_indices])\n",
    "X_train_subsampled = X_train[subsample_indices]\n",
    "y_train_subsampled = y_train[subsample_indices]\n",
    "print(X_train_subsampled.shape, y_train_subsampled.shape)\n",
    "\n",
    "shuffle_indices = np.random.permutation(len(X_train_subsampled))\n",
    "X_train_subsampled = X_train_subsampled[shuffle_indices]\n",
    "y_train_subsampled = y_train_subsampled[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# rf_model = RandomForestClassifier()\n",
    "# rf_model.fit(X_train_subsampled, y_train_subsampled)\n",
    "\n",
    "# # Get prediction probabilities\n",
    "# predictions = rf_model.predict_proba(X_test)\n",
    "# results = []\n",
    "\n",
    "# # Iterate over thresholds from 0.1 to 0.96 with a step of 0.02\n",
    "# for threshold in np.arange(0.1, 0.96, 0.02):\n",
    "#     y_pred = ['1' if p[1] >= threshold else '0' for p in predictions]\n",
    "    \n",
    "#     mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    \n",
    "#     tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=['0', '1']).ravel()\n",
    "    \n",
    "#     sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "#     specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    \n",
    "#     results.append((threshold, mcc, sensitivity, specificity))\n",
    "\n",
    "\n",
    "# results_df = pd.DataFrame(results, columns=['Threshold', 'MCC', 'Sensitivity', 'Specificity'])\n",
    "# results_df = results_df.round(2)\n",
    "# styled_results_df = results_df.style.format(\"{:.2f}\").apply(highlight_max, subset=['MCC'])\n",
    "# display(styled_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# rf_model = RandomForestClassifier(random_state=42)\n",
    "# rf_model.fit(X_train_subsampled, y_train_subsampled)\n",
    "\n",
    "# threshold = 0.5\n",
    "\n",
    "# # Get prediction probabilities\n",
    "# predictions = rf_model.predict_proba(X_test)\n",
    "# y_pred = [1 if p[1] >= threshold else 0 for p in predictions]\n",
    "# print(y_pred)\n",
    "\n",
    "# phi_threshold = 0.5\n",
    "# scaling_const = 50\n",
    "\n",
    "# d_score = np.abs(predictions[:, 1] - phi_threshold)\n",
    "# # print(d_score)\n",
    "# reliability_index = np.clip(scaling_const * (d_score), -1, 10)\n",
    "\n",
    "# y_pred = np.array(y_pred)\n",
    "\n",
    "# # get all correctly predicted and incorrectly predicted positive indices\n",
    "# correct_positive_indices = np.where((y_test == '1') & (y_pred == 1))[0]\n",
    "# incorrect_positive_indices = np.where((y_test == '1') & (y_pred == 0))[0]\n",
    "# print(correct_positive_indices.shape, incorrect_positive_indices.shape)\n",
    "\n",
    "# correct_positive_ri = reliability_index[correct_positive_indices]\n",
    "# incorrect_positive_ri = reliability_index[incorrect_positive_indices]\n",
    "\n",
    "# correct_negative_indices = np.where((y_test == '0') & (y_pred == 0))[0]\n",
    "# incorrect_negative_indices = np.where((y_test == '0') & (y_pred == 1))[0]\n",
    "# print(correct_negative_indices.shape, incorrect_negative_indices.shape)\n",
    "\n",
    "# correct_negative_ri = reliability_index[correct_negative_indices]\n",
    "# incorrect_negative_ri = reliability_index[incorrect_negative_indices]\n",
    "\n",
    "# # Do a boxplot of the reliability index for correctly predicted and incorrectly predicted positive samples\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.boxplot([correct_positive_ri, incorrect_positive_ri, correct_negative_ri, incorrect_negative_ri], labels=['TP', 'FP', 'TN', 'FN'])\n",
    "# plt.xlabel('Prediction Outcome')\n",
    "# plt.ylabel('Reliability Index')\n",
    "# plt.title('Reliability Index')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Plot a graph of accuracy against reliability index\n",
    "# reliability_index = np.round(np.array(reliability_index))\n",
    "# accuracies = []\n",
    "# ri_sample_percentages = []\n",
    "\n",
    "# for ri in range(0, 11):\n",
    "#     # get all indices in reliability index with score ri\n",
    "#     ri_indices = np.where(reliability_index == ri)[0]\n",
    "#     # get the predictions for these indices\n",
    "#     ri_predictions = np.array(y_pred[ri_indices]).astype(int)\n",
    "#     # get the actual labels for these indices\n",
    "#     ri_actual = np.array(y_test[ri_indices]).astype(int)\n",
    "\n",
    "#     ri_sample_percentages.append(np.round(ri_indices.shape[0] / y_test.shape[0] * 100, 1))\n",
    "    \n",
    "#     # calculate the accuracy for these indices\n",
    "#     accuracy = accuracy_score(ri_actual, ri_predictions)\n",
    "#     accuracies.append(accuracy)\n",
    "    \n",
    "# accuracies = np.round(np.array(accuracies), 2)\n",
    "# print(accuracies)\n",
    "# print(ri_sample_percentages)\n",
    "\n",
    "# # for ri in range(0, 11):\n",
    "# #     # get all indices in reliability index with score ri\n",
    "# #     ri_indices = np.where((reliability_index == ri) & (y_test == '1'))[0]\n",
    "# #     # get the predictions for these indices\n",
    "# #     ri_predictions = np.array(y_pred[ri_indices]).astype(int)\n",
    "# #     # get the actual labels for these indices\n",
    "# #     ri_actual = np.array(y_test[ri_indices]).astype(int)\n",
    "\n",
    "# #     ri_sample_percentages.append(np.round(ri_indices.shape[0] / y_test.shape[0] * 100, 1))\n",
    "    \n",
    "# #     # calculate the accuracy for these indices\n",
    "# #     accuracy = accuracy_score(ri_actual, ri_predictions)\n",
    "# #     accuracies.append(accuracy)\n",
    "    \n",
    "# # accuracies = np.round(np.array(accuracies), 2)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(np.arange(0, 11), accuracies, marker = 'o')\n",
    "# plt.title('Accuracy vs Reliability Index')\n",
    "# plt.xlabel('Reliability Index')\n",
    "# plt.ylabel('Accuracy (%)')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# add another x-axis for the percentage of samples in each reliability index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cpu\") # \"cuda\"\n",
    " \n",
    "# # class SimpleCNN(nn.Module):\n",
    "# #     def __init__(self):\n",
    "# #         super(SimpleCNN, self).__init__()\n",
    "\n",
    "# #         # Branch for the 125D input vector\n",
    "# #         self.branch_54D = nn.Sequential(\n",
    "# #             nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
    "# #             nn.ReLU(),\n",
    "# #             nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "# #             nn.ReLU(),\n",
    "# #             nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "# #         )\n",
    "\n",
    "# #         # Branch for the 8D input vector\n",
    "# #         self.branch_8D = nn.Sequential(\n",
    "# #             nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
    "# #             nn.ReLU(),\n",
    "# #             nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "# #             nn.ReLU(),\n",
    "# #             nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "# #         )\n",
    "\n",
    "# #         # Combined branch\n",
    "# #         combined_input_size = 32 * 27 + 32 * 4\n",
    "# #         self.combined_branch = nn.Sequential(\n",
    "# #             nn.Linear(combined_input_size, 128),\n",
    "# #             nn.ReLU(),\n",
    "# #             nn.Linear(128, 64),\n",
    "# #             nn.ReLU(),\n",
    "# #             nn.Linear(64, 1)  # Adjust output size based on your needs\n",
    "# #         )\n",
    "\n",
    "# #     def forward(self, x_125D, x_8D):\n",
    "# #         # Process 125D input vector\n",
    "# #         x1 = x_125D.unsqueeze(1)  # Add channel dimension\n",
    "# #         x1 = self.branch_54D(x1)\n",
    "# #         x1 = x1.view(x1.size(0), -1)  # Flatten\n",
    "\n",
    "# #         # Process 8D input vector\n",
    "# #         x2 = x_8D.unsqueeze(1)  # Add channel dimension\n",
    "# #         x2 = self.branch_8D(x2)\n",
    "# #         x2 = x2.view(x2.size(0), -1)  # Flatten\n",
    "\n",
    "# #         # Combine the two branches\n",
    "# #         x_combined = torch.cat((x1, x2), dim=1)\n",
    "# #         output = self.combined_branch(x_combined)\n",
    "\n",
    "# #         return output\n",
    "\n",
    "# num_epochs = 10\n",
    "\n",
    "# class SimpleCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SimpleCNN, self).__init__()\n",
    "\n",
    "#         # Define branches for each vector\n",
    "#         self.branch_54D = nn.Sequential(\n",
    "#             nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "#             nn.Dropout(0.5)\n",
    "#         )\n",
    "        \n",
    "#         self.branch_8D = nn.Sequential(\n",
    "#             nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "#             nn.Dropout(0.5)\n",
    "#         )\n",
    "\n",
    "#         self.branch_63D = nn.Sequential(\n",
    "#             nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "#             nn.Dropout(0.5)\n",
    "#         )\n",
    "        \n",
    "#         combined_input_size = (32 * 62) + (32 * 4) + (32 * 30)\n",
    "        \n",
    "#         self.combined_branch = nn.Sequential(\n",
    "#             nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "#             nn.Dropout(0.5)\n",
    "#         )\n",
    "        \n",
    "#         # Output size must be adjusted according to the actual output size after combined_branch\n",
    "#         self.linear = nn.Sequential(\n",
    "#             nn.Linear(15872 , 1024),  # Assuming 64 channels with a length of 30 after pooling and dropout\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(1024, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(128, 1)  # Output layer, adjust according to your needs\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x_54D, x_8D, x_63D):\n",
    "#         x1 = self.branch_54D(x_54D.unsqueeze(1))\n",
    "#         x2 = x_8D.unsqueeze(1)\n",
    "#         x3 = self.branch_63D(x_63D.unsqueeze(1))\n",
    "\n",
    "#         # Flatten and concatenate\n",
    "#         x1 = x1.view(x1.size(0), -1)\n",
    "#         x2 = x2.view(x2.size(0), -1)\n",
    "#         x3 = x3.view(x3.size(0), -1)\n",
    "#         x_combined = torch.cat((x1, x2, x3), dim=1)\n",
    "\n",
    "#         # Combined convolutional processing\n",
    "#         x_combined = x_combined.view(x_combined.size(0), 1, -1)  # Reshape for Conv1d input\n",
    "#         x_combined = self.combined_branch(x_combined)\n",
    "#         x_combined = x_combined.view(x_combined.size(0), -1)\n",
    "\n",
    "#         # Final linear layers\n",
    "#         output = self.linear(x_combined)\n",
    "        \n",
    "#         return output\n",
    "# def train_cnn():\n",
    "#     for epoch in range(num_epochs):\n",
    "#         cnn.train()  # Set the model to training mode\n",
    "#         running_loss = 0.0\n",
    "        \n",
    "#         for X_54D_batch, X_8D_batch, X_63D_batch, y_batch in train_loader:\n",
    "#             # X_54D_batch, y_batch = X_54D_batch.to(device), y_batch.to(device)\n",
    "#             X_54D_batch, X_8D_batch, X_63D_batch, y_batch = X_54D_batch.to(device), X_8D_batch.to(device), X_63D_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()  # Zero the parameter gradients\n",
    "            \n",
    "#             # Forward pass\n",
    "#             # outputs = cnn(X_54D_batch)\n",
    "#             outputs = cnn(X_54D_batch, X_8D_batch, X_63D_batch)\n",
    "#             outputs = outputs.squeeze()  # Remove the extra dimension for loss calculation\n",
    "            \n",
    "#             # Compute the loss\n",
    "#             loss = criterion(outputs, y_batch)\n",
    "            \n",
    "#             # Backward pass and optimization\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             # Accumulate loss\n",
    "#             running_loss += loss.item() * X_54D_batch.size(0)\n",
    "        \n",
    "#         epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# X_train_54D = torch.tensor(X_train_subsampled[:, :54], dtype=torch.float32)\n",
    "# X_train_8D = torch.tensor(X_train_subsampled[:, 54:62], dtype=torch.float32)\n",
    "# X_train_63D = torch.tensor(X_train_subsampled[:, 62:], dtype=torch.float32)\n",
    "\n",
    "# # X_train = torch.tensor(X_train_subsampled, dtype=torch.float32)\n",
    "\n",
    "# y_train = torch.tensor(y_train_subsampled.astype(int), dtype=torch.float32) \n",
    "# # train_dataset = TensorDataset(X_train, y_train)\n",
    "# train_dataset = TensorDataset(X_train_54D, X_train_8D, X_train_63D, y_train)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# cnn = SimpleCNN()\n",
    "# cnn.to(device)\n",
    "\n",
    "# # Define loss function and optimizer\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "# train_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_subsampled = y_train_subsampled.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6335\n",
      "Epoch 2/10, Loss: 0.5816\n",
      "Epoch 3/10, Loss: 0.5748\n",
      "Epoch 4/10, Loss: 0.5667\n",
      "Epoch 5/10, Loss: 0.5578\n",
      "Epoch 6/10, Loss: 0.5525\n",
      "Epoch 7/10, Loss: 0.5380\n",
      "Epoch 8/10, Loss: 0.5334\n",
      "Epoch 9/10, Loss: 0.5210\n",
      "Epoch 10/10, Loss: 0.5067\n",
      "Accuracy: 0.6651\n",
      "MCC: 0.1766\n",
      "Sensitivity: 0.7222\n",
      "Specificity: 0.6620\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(143, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create the model, criterion, and optimizer\n",
    "model = SimpleNN()\n",
    "criterion = nn.BCEWithLogitsLoss()  # This combines a Sigmoid layer and the BCELoss\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Assuming X_train_subsampled and y_train_subsampled are numpy arrays\n",
    "X_train_nn = torch.tensor(X_train_subsampled, dtype=torch.float32)\n",
    "y_train_nn = torch.tensor(y_train_subsampled, dtype=torch.float32)\n",
    "train_dataset = TensorDataset(X_train_nn, y_train_nn)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training function\n",
    "def train_nn():\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch).squeeze() \n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "    \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Testing function with MCC, Sensitivity, and Specificity\n",
    "def test_nn():\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            predicted = torch.round(torch.sigmoid(outputs))  # Apply Sigmoid and round to get binary output\n",
    "            all_preds.extend(predicted.tolist())\n",
    "            all_targets.extend(y_batch.tolist())\n",
    "    \n",
    "    # Convert to tensors\n",
    "    all_preds = torch.tensor(all_preds)\n",
    "    all_targets = torch.tensor(all_targets)\n",
    "    \n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(all_targets, all_preds)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(all_targets, all_preds).ravel()\n",
    "    \n",
    "    # Calculate Sensitivity and Specificity\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'MCC: {mcc:.4f}')\n",
    "    print(f'Sensitivity: {sensitivity:.4f}')\n",
    "    print(f'Specificity: {specificity:.4f}')\n",
    "\n",
    "# Train and test the model\n",
    "train_nn()\n",
    "test_nn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test our cnn model\n",
    "# # X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# X_test_54D = torch.tensor(X_test[:, :54], dtype=torch.float32)\n",
    "# X_test_8D = torch.tensor(X_test[:, 54:62], dtype=torch.float32)\n",
    "# X_test_63D = torch.tensor(X_test[:, 62:], dtype=torch.float32)\n",
    "# y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# test_dataset = TensorDataset(X_test_54D, X_test_8D, X_test_63D, y_test)\n",
    "# # test_dataset = TensorDataset(X_test, y_test)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# def test_cnn():\n",
    "#     cnn.eval()  # Set the model to evaluation mode\n",
    "#     test_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     all_predictions = []\n",
    "#     all_targets = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for X_batch_54D, X_batch_8D, X_batch_63D, y_batch in test_loader:\n",
    "#             # X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#             X_batch_54D, X_batch_8D, X_batch_63D, y_batch = X_batch_54D.to(device), X_batch_8D.to(device), X_batch_63D.to(device), y_batch.to(device)   \n",
    "\n",
    "#             outputs = cnn(X_batch_54D, X_batch_8D, X_batch_63D)\n",
    "#             outputs = outputs.squeeze()\n",
    "\n",
    "#             predicted = torch.round(torch.sigmoid(outputs))\n",
    "#             all_predictions.extend(predicted.cpu().numpy())\n",
    "#             all_targets.extend(y_batch.cpu().numpy())\n",
    "#             total += y_batch.size(0)\n",
    "#             correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "#     test_accuracy = correct / total\n",
    "#     mcc = matthews_corrcoef(all_targets, all_predictions)\n",
    "\n",
    "#     tn, fp, fn, tp = confusion_matrix(all_targets, all_predictions).ravel()\n",
    "#     sensitivity = tp / (tp + fn)\n",
    "#     specificity = tn / (tn + fp)\n",
    "\n",
    "#     print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "#     print(f'Test MCC: {mcc:.4f}')\n",
    "#     print(f'Test Sensitivity: {sensitivity:.4f}')\n",
    "#     print(f'Test Specificity: {specificity:.4f}')\n",
    "\n",
    "# test_cnn()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
