{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import pickle\n",
    "# from line_profiler import LineProfiler\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score, confusion_matrix, recall_score, f1_score\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.utils import shuffle\n",
    "# from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pfam2go import pfam2go \n",
    "from typing import Iterable, Optional\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Physicochemical Properties\n",
    "\n",
    "Most sources can be found on AAindex\n",
    "\n",
    "PKA source: D.R. Lide, Handbook of Chemistry and Physics, 72nd Edition, CRC Press, Boca Raton, FL, 1991. (Sigma Aldrich website)\n",
    "\n",
    "EIIP: Electron-ion interaction potential (Veljkovic et al., 1985)\n",
    "\n",
    "LEP: No citation, sorta implicit (NOT VERIFIED!)\n",
    "\n",
    "Wiener Index: ?\n",
    "\n",
    "Molecular Mass: Wikipedia, implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMINO_ACID_INDICES = {'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'Q': 5, 'E': 6, 'G': 7, 'H': 8, 'I': 9, \n",
    "                      'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19}\n",
    "\n",
    "PKA_AMINO_GROUP = np.array([9.69, 9.04, 8.80, 9.60, 10.28, 9.13, 9.67, 9.60, 9.17, 9.60,\n",
    "                            9.60, 8.95, 9.21, 9.13, 10.60, 9.15, 9.10, 9.39, 9.11, 9.62])\n",
    "PKA_CARBOXYL_GROUP = np.array([2.34, 2.17, 2.02, 1.88, 1.96, 2.17, 2.19, 2.34, 1.82, 2.36,\n",
    "                               2.36, 2.18, 2.28, 1.83, 1.99, 2.21, 2.09, 2.83, 2.20, 2.32])\n",
    "EIIP = np.array([0.03731, 0.09593, 0.00359, 0.12630, 0.08292, 0.07606, 0.00580, 0.00499, 0.02415, 0.0000, \n",
    "                 0.0000, 0.03710, 0.08226, 0.09460, 0.01979, 0.08292, 0.09408, 0.05481, 0.05159, 0.00569])\n",
    "LONE_ELECTRON_PAIRS = np.array([0, 0, 1, 2, 1, 1, 2, 0, 1, 0, \n",
    "                                0, 0, 0, 0, 0, 1, 1, 0, 1, 0])\n",
    "WIENER_INDEX = np.array([0.3466, 0.1156, 0.3856, 0.2274, 0.0501, 0.6379, 0.1938, 0.1038, 0.2013,\n",
    "                       0.2863, 0.1071, 0.7767, 0.7052, 0.3419, 0.0957, 0.4375, 0.9320, 0.1000, 0.1969, 0.9000])\n",
    "MOLECULAR_MASS = np.array([89.094, 174.203, 132.119, 133.104, 121.154, 146.146, 147.131, 75.067, 155.156, 131.175,\n",
    "                           131.175, 146.189, 149.208, 165.192, 115.132, 105.093, 119.119, 204.228, 181.191, 117.148])\n",
    "\n",
    "PP_LIST = [PKA_AMINO_GROUP, PKA_CARBOXYL_GROUP, EIIP, LONE_ELECTRON_PAIRS, WIENER_INDEX, MOLECULAR_MASS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amino Acid Composition (AAC) groups - Polarity Charge\n",
    "# C1; C2; C3; C4 \n",
    "# (polar amino acid with positive charge, polar amino acid with negative charge, noncharged\n",
    "# polar amino acid, nonpolar amino acid).\n",
    "\n",
    "AAC_C1 = ['G', 'A', 'V', 'L', 'I', 'F', 'W', 'M', 'P']\n",
    "AAC_C2 = ['S', 'T', 'C', 'Y', 'N', 'Q']\n",
    "AAC_C3 = ['D', 'E']\n",
    "AAC_C4 = ['R', 'K', 'H']\n",
    "\n",
    "AAC_C_LIST = [AAC_C1, AAC_C2, AAC_C3, AAC_C4]\n",
    "\n",
    "# Amino Acid Composition (AAC) groups - Hydrohpobicity\n",
    "# H1;H2;H3;H4  (strong hydrophobic residue, weak hydrophobic residue, strong hydrophilic residue, weak hydrophilic residue).\n",
    "# This scale is obtained from Kyte and Doolittle (1982). \n",
    "# K&D scale from 0 to +-2.0 is considered weak, >2.0 is strong hydrophobicity, and <-2.0 is strong hydrophilic. \n",
    "\n",
    "\n",
    "AAC_H1 = ['I', 'V', 'L', 'F', 'C']\n",
    "AAC_H2 = ['M', 'A']\n",
    "AAC_H3 = ['H', 'Q', 'N', 'E', 'D', 'K', 'R']\n",
    "AAC_H4 = ['G', 'T', 'S', 'W', 'Y', 'P']\n",
    "\n",
    "AAC_H_LIST = [AAC_H1, AAC_H2, AAC_H3, AAC_H4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GO Terms List \n",
    "\n",
    "The json file provides a list of GO terms that are potentially invovled in DNA-binding\n",
    "\n",
    "Most are descendants of DNA-binding (\"GO:0003677\") and DNA-binding transcription factor activity (\"DNA-binding transcription factor activity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNA_BINDING_GO_TERM_LIST = json.load(open(\"descendant_ids.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PP Matrix stored as a constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows: normalized pp properties \n",
    "# columns: amino acids\n",
    "def create_pp_matrix() -> np.ndarray:\n",
    "    pp_matrix = np.empty((len(PP_LIST), len(AMINO_ACID_INDICES)), dtype=float)\n",
    "    for i, pp in enumerate(PP_LIST):\n",
    "        max_val = np.max(pp)\n",
    "        min_val = np.min(pp)\n",
    "        pp_matrix[i] = (pp - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return pp_matrix\n",
    "\n",
    "# Constant PP_MATRIX\n",
    "PP_MATRIX = create_pp_matrix()\n",
    "# print(PP_MATRIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBV\n",
    "\n",
    "Source: Shen, Juwen, et al. \"Predicting proteinâ€“protein interactions based only on sequences information.\" Proceedings of the National Academy of Sciences 104.11 (2007): 4337-4341. (Supp. information)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obv_classes = {\n",
    "    'A' : 0, 'G' : 0, 'V' : 0,\n",
    "    'I': 1, 'L': 1, 'F': 1, 'P': 1,\n",
    "    'Y': 2, 'M': 2, 'T': 2, 'S': 2,\n",
    "    'H': 3, 'N': 3, 'Q': 3, 'W': 3,\n",
    "    'R': 4, 'K': 4,\n",
    "    'D': 5, 'E': 5,\n",
    "    'C': 6\n",
    "}\n",
    "\n",
    "def generate_obv(amino_acid):\n",
    "    temp = np.zeros(7)\n",
    "    temp[obv_classes.get(amino_acid)] = 1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Window Instance from sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a string, then \n",
    "# extract list of instances by sliding a window through the sequence\n",
    "def get_instances_from_seq(seq : str, window_size : int = 9) -> list :\n",
    "    instances = list()\n",
    "    for i in range(len(seq) - window_size + 1):\n",
    "        instances.append(seq[i:i+window_size])\n",
    "    return instances\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate PSSM-PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate PSSM using psiblast from a given sequence.\"\"\"\n",
    "def generate_pssm(input_seq: str, num_iterations = 3) -> np.ndarray:\n",
    "    DB_PATH = \"./databases/uniprot_sprot.fasta\"\n",
    "    output_pssm = \"output.pssm\"\n",
    "\n",
    "    # Creating a temporary fasta file for input\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".fasta\") as temp_fasta:\n",
    "        SeqIO.write([SeqRecord(Seq(input_seq))], temp_fasta, \"fasta\")\n",
    "        temp_fasta_path = temp_fasta.name\n",
    "\n",
    "    # Running psiblast\n",
    "    try:\n",
    "        subprocess.run([\"psiblast\", \"-query\", temp_fasta_path, \"-db\", DB_PATH, \n",
    "                        \"-out_ascii_pssm\", output_pssm, \"-num_iterations\", str(num_iterations), \"-evalue\", \"0.001\"], \n",
    "                        check=True, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "    finally:\n",
    "        os.remove(temp_fasta_path)\n",
    "\n",
    "    # Reading PSSM output\n",
    "    try:\n",
    "        pssm_df = pd.read_csv(output_pssm, delim_whitespace=True, skiprows=3, header=None)\n",
    "        os.remove(output_pssm)  # Clean up PSSM file after reading\n",
    "        pssm_array = pssm_df.iloc[:-5, 2:22].to_numpy(dtype=int)\n",
    "        return pssm_array\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: PSSM file not found. Input Sequence: {input_seq}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "# Rescale pssm using sigmoid\n",
    "def rescale_pssm(input_pssm) -> np.ndarray:\n",
    "    input_pssm = 1/(1 + np.exp(-input_pssm))\n",
    "    return input_pssm\n",
    "\n",
    "# Get only the pssm rows that are relevant to the sequence\n",
    "def get_sliced_pssm(original_pssm : np.ndarray, start_index : int, window_size : int = 9):\n",
    "    return original_pssm[start_index : start_index + window_size, :]\n",
    "\n",
    "def create_pssm_pp(pssm_matrix : np.ndarray, pp_matrix : np.ndarray) -> np.ndarray:\n",
    "    pssm_matrix = np.array(pssm_matrix, dtype=float)\n",
    "    return pp_matrix @ pssm_matrix\n",
    "\n",
    "def get_all_slices(input_pssm: np.ndarray, window_size: int):\n",
    "    slices = []\n",
    "    for i in range(input_pssm.shape[0] - window_size + 1):\n",
    "        slices.append(input_pssm[i: i + window_size, :])\n",
    "    return slices\n",
    "\n",
    "\n",
    "# file = list(pickle.load(open(\"generated_pssms_train.pkl\", \"rb\")))\n",
    "# print(np.array(file[0]).shape)\n",
    "# print(len(get_all_slices(np.random.rand(21, 20), 9)))\n",
    "\n",
    "# generate_pssm(\"KPKNKDKDKKVPEPDNKKKKPKKEEEQKWKWWEEERYPEGIKWKFLEHKGPVFAPPYEPLPENVKFYYDGKVMKLSPKAEEVATFFAKMLDHEYTTKEIFRKNFFKDWRKEMTNEEKNIITNLSKCDFTQMSQYFKAQTEARKQMSKEEKLKIKEENEKLLKEYGFCIMDNHKERIANFKIEPPGLFRGRGNHPKMGMLKRRIMPEDIIINCSKDAKVPSPPPGHKWKEVRHDNKVTWLVSWTENIQGSIKYIMLNPSSRIKGEKDWQKYETARRLKKCVDKIRNQYREDWKSKEMKVRQRAVALYFIDKLALRAGNEKEEGETADTVGCCSLRVEHINLHPELDGQEYVVEFDFLGKDSIRYYNKVPVEKRVFKNLQLFMENKQPEDDLFDRLNTGILNKHLQDLMEGLTAKVFRTYNASITLQQQLKELTAPDENIPAKILSYNRANRAVAILCNHQRAPPKTFEKSMMNLQTKIDAKKEQLADARRDLKSAKADAKVMKDAKTKKVVESKKKAVQRLEEQLMKLEVQATDREENKQIALGTSKLNFLDPRITVAWCKKWGVPIEKIYNKTQREKFAWAIDMADEDYE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amino Acid Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AAC_PC takes in a sequence of 9 amino acids then outputs a list of 4 values\n",
    "def calculate_AAC_PC(seq : str):\n",
    "    window_size = len(seq)\n",
    "    \n",
    "    def get_c_i():\n",
    "        c_i = np.zeros((4, window_size - 1), dtype=int)\n",
    "        for gap in range(1, window_size):\n",
    "            for j in range(window_size - gap):\n",
    "                for index, aac_class in enumerate(AAC_C_LIST):\n",
    "                    if seq[j] in aac_class and seq[j + gap] in aac_class:\n",
    "                        c_i[index][gap - 1] += 1\n",
    "        # print(c_i)\n",
    "        return c_i\n",
    "    \n",
    "    def get_n_i():\n",
    "        n_i = [np.sum(seq.count(a) for a in aac_class) for aac_class in AAC_C_LIST]\n",
    "        # print(n_i)\n",
    "        return np.array(n_i)\n",
    "    \n",
    "    c_i = get_c_i()\n",
    "    n_i = get_n_i()\n",
    "    \n",
    "    output_aac_list = list()\n",
    "    for i in range(0, 4):\n",
    "        sum = 0\n",
    "        for k in range(0, window_size - 1):\n",
    "            first_term = ((c_i[i][k] / (window_size - k)) - (n_i[i]**2 / window_size**2))\n",
    "            if np.isnan(first_term):\n",
    "                first_term = 0\n",
    "            second_term = np.square(first_term) / (2 * (n_i[i]**2 / window_size**2))\n",
    "            if np.isnan(second_term):\n",
    "                second_term = 0\n",
    "            sum += (first_term + second_term)\n",
    "        output_aac_list.append(sum)\n",
    "    \n",
    "    # print(output_aac_list)\n",
    "    return output_aac_list\n",
    "\n",
    "def calculate_AAC_H(seq : str):\n",
    "    window_size = len(seq)\n",
    "    def get_h_i():\n",
    "        h_i = np.zeros((4, window_size - 1), dtype=int)\n",
    "        for gap in range(1, window_size):\n",
    "            for j in range(window_size - gap):\n",
    "                for index, aac_class in enumerate(AAC_H_LIST):\n",
    "                    if seq[j] in aac_class and seq[j + gap] in aac_class:\n",
    "                        h_i[index][gap - 1] += 1\n",
    "        # print(h_i)\n",
    "        return h_i\n",
    "    \n",
    "    def get_m_i():\n",
    "        m_i = [np.sum(seq.count(a) for a in aac_class) for aac_class in AAC_H_LIST]\n",
    "        # print(m_i)\n",
    "        return np.array(m_i)\n",
    "    \n",
    "    h_i = get_h_i()\n",
    "    m_i = get_m_i()\n",
    "    \n",
    "    output_aac_list = list()\n",
    "    for i in range(0, 4):\n",
    "        sum = 0\n",
    "        for k in range(0, window_size - 1):\n",
    "            first_term = ((h_i[i][k] / (window_size - k)) - (m_i[i]**2 / window_size**2))\n",
    "            first_term = 0 if np.isnan(first_term) else first_term\n",
    "            second_term = np.square(first_term) / (2 * (m_i[i]**2 / window_size**2))\n",
    "            second_term = 0 if np.isnan(second_term) else second_term\n",
    "            sum += (first_term + second_term)\n",
    "        output_aac_list.append(sum)\n",
    "    \n",
    "    # print(output_aac_list)\n",
    "    return output_aac_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_obv(seq: str):\n",
    "    full_obv = np.zeros((len(seq), 7)) \n",
    "    for idx, aa in enumerate(seq):\n",
    "        full_obv[idx] = generate_obv(aa)  \n",
    "    return full_obv.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-generate PSSMs and store to a numpy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pssms for a list of sequences, then save them to a pickle file for future use\n",
    "def pre_generate_pssm(input_df, file_name:str, window_size : int):\n",
    "    pssm_list = list()\n",
    "    for seq in input_df['seq']:\n",
    "        pssm = generate_pssm(seq)\n",
    "        pssm_list.append(pssm)\n",
    "    \n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(pssm_list, f)\n",
    "    return pssm_list\n",
    "    \n",
    "# list_of_train_pssms = pre_generate_pssm(pd.read_csv(\"./DRNA_TRAIN.csv\"), \"generated_pssms_train_15.pkl\", 15)\n",
    "# print(len(list_of_train_pssms))\n",
    "# list_of_test_pssms = pre_generate_pssm(pd.read_csv(\"./DRNA_TEST.csv\"), \"generated_pssms_test_15.pkl\", 15)\n",
    "# print(len(list_of_test_pssms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disorder prediction (IUPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen_disorder_pred(input_seq: str) -> np.ndarray:\n",
    "#     # create temporary fasta file based on input seq\n",
    "#     with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".fasta\") as temp_fasta:\n",
    "#         SeqIO.write([SeqRecord(Seq(input_seq))], temp_fasta, \"fasta\")\n",
    "#         temp_fasta_path = temp_fasta.name\n",
    "    \n",
    "#     # run IUPred, save stdout to a file\n",
    "#     with open(\"disorder_pred.txt\", \"w\") as output_file:\n",
    "#         subprocess.run([\"python3\", \"iupred3/iupred3.py\", temp_fasta_path, \"long\"], check=True, stdout=output_file, stderr=subprocess.STDOUT)\n",
    "    \n",
    "#     # parse output file and convert to a numpy array\n",
    "#     with open(\"disorder_pred.txt\", \"r\") as output_file:\n",
    "#         lines = output_file.readlines()\n",
    "#         # Get line 13 onwards, and only the 3rd column\n",
    "#         disorder_pred = np.array([float(line.split()[2]) for line in lines[12:]])\n",
    "        \n",
    "#     # clean up files\n",
    "#     os.remove(temp_fasta_path)\n",
    "#     return disorder_pred\n",
    "\n",
    "# def gen_disorder_pred_for_dataset(input_file):\n",
    "#     df = pd.read_csv(input_file)\n",
    "#     disorder_preds = list()\n",
    "#     for seq in df['seq']:\n",
    "#         disorder_pred = gen_disorder_pred(seq)\n",
    "#         disorder_preds.append(disorder_pred)\n",
    "    \n",
    "#     # save to pickle file\n",
    "#     with open(\"disorder_preds.pkl\", \"wb\") as f:\n",
    "#         pickle.dump(disorder_preds, f)\n",
    "        \n",
    "#     return disorder_preds\n",
    "\n",
    "# def get_diso_pred_for_seq(seq_index: int):\n",
    "#     with open(\"disorder_preds.pkl\", \"rb\") as f:\n",
    "#         disorder_preds = pickle.load(f)\n",
    "#     return disorder_preds[seq_index]\n",
    "\n",
    "# print(gen_disorder_pred_for_dataset(\"./DRNA_TEST.csv\"))\n",
    "\n",
    "# print(gen_disorder_pred(\"MIEIKDKQLTGLRFIDLFAGLGGFRLALESCGAECVYSNEWDKYAQEVYEMNFGEKPEGDITQVNEKTIPDHDILCAGFPCQAFSISGKQKGFEDSRGTLFFDIARIVREKKPKVVFMENVKNFASHDNGNTLEVVKNTMNELDYSFHAKVLNALDYGIPQKRERIYMICFRNDLNIQNFQFPKPFELNTFVKDLLLPDSEVEHLVIDRKDLVMTNQEIEQTTPKTVRLGIVGKGGQGERIYSTRGIAITLSAYGGGIFAKTGGYLVNGKTRKLHPRECARVMGYPDSYKVHPSTSQAYKQFGNSVVINVLQYIAYNIGSSLNFKPY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Features via HMMER\n",
    "\n",
    "1. Run sequence using HMMER hmmscan \n",
    "1. Parse output file to obtain domains\n",
    "1. Map the domains to GO terms\n",
    "1. From all GO terms, identify if any of the terms are DNA-binding\n",
    "1. If yes, residues within the Pfam domain are labeled as DNA-binding\n",
    "\n",
    "### Pfam Domain to GO Term\n",
    "\n",
    "We want to get all (at least most) DNA-binding domains. However, Pfam domains do not explicitly state that a certain domain is DNA binding. However, we can map these Pfam domains to GO terms, which indicates if a domain is DNA-binding or not.\n",
    "\n",
    "This is done via pfam2go, an external library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hmmscan(input_seq: str):\n",
    "    \"\"\"Run hmmscan against a given sequence.\"\"\"\n",
    "    with tempfile.NamedTemporaryFile(mode='w+', suffix='.fasta') as temp_fasta:\n",
    "        SeqIO.write([SeqRecord(Seq(input_seq), id='Query')], temp_fasta, 'fasta')\n",
    "        temp_fasta.seek(0)\n",
    "        output_file = 'r_d.out'\n",
    "        subprocess.run(['hmmscan', '--domtblout', output_file, '--domE', '1e-05', 'pfam/Pfam-A.hmm', temp_fasta.name],\n",
    "                       check=True, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "        return output_file\n",
    "\n",
    "def parse_hmmscan_output(domtblout: str):\n",
    "    \"\"\"Parse the hmmscan output to extract relevant data.\"\"\"\n",
    "    if not os.path.exists(domtblout):\n",
    "        print(\"Output file not found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    with open(domtblout, 'r') as f:\n",
    "        lines = [line for line in f if not line.startswith('#')]\n",
    "\n",
    "    if not lines:\n",
    "        print(\"No Pfam IDs found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(lines, columns=['line'])\n",
    "    df = df['line'].str.split(expand=True)\n",
    "    df = df.iloc[:, [1, 3, 5, 17, 18]].rename(columns={1: \"pfam_id\", 3: \"query_id\", 5: \"qlen\",  17: \"start\", 18: \"end\"})\n",
    "    df['pfam_id'] = df['pfam_id'].str.split('.').str[0]\n",
    "    df[['start', 'end']] = df[['start', 'end']].astype(int)\n",
    "    return df\n",
    "\n",
    "def add_go_terms(input_df : pd.DataFrame) -> pd.DataFrame:\n",
    "    if input_df.empty:\n",
    "        print(\"No Pfam IDs found, input is empty Dataframe\")\n",
    "        input_df['go_terms'] = None\n",
    "        return input_df\n",
    "    \n",
    "    pfam_list = list([x for x in input_df.iloc[:, 0]])\n",
    "    result = pfam2go(pfam_list)\n",
    "    # print(result)\n",
    "    \n",
    "    if not pfam_list or result is None:\n",
    "        print(\"No Pfam IDs found, input is empty Dataframe\")\n",
    "        input_df['go_terms'] = None\n",
    "        return input_df\n",
    "\n",
    "    # pfam_to_go = dict(zip(result[\"Pfam accession\"], result[\"GO accession\"]))\n",
    "    result = result.groupby('Pfam accession')['GO accession'].agg(list).reset_index()\n",
    "    pfam_to_go = dict(zip(result[\"Pfam accession\"], result[\"GO accession\"]))\n",
    "    # print(pfam_to_go)\n",
    "    \n",
    "    input_df['go_terms'] = input_df.iloc[:, 0].apply(lambda x: pfam_to_go.get(x.split('.')[0], ''))\n",
    "    # print(input_df)\n",
    "    return input_df\n",
    "\n",
    "def label_dna_binding(input_df : pd.DataFrame, seq_len : int) -> np.ndarray:\n",
    "    label_array = np.zeros(seq_len) \n",
    "    \n",
    "    if input_df.empty:\n",
    "        print(\"No GO terms found\")\n",
    "        return label_array\n",
    "    \n",
    "    input_df['is_dna_binding'] = input_df['go_terms'].apply(\n",
    "        lambda go_terms: 1 if go_terms is not None and any(go in DNA_BINDING_GO_TERM_LIST for go in go_terms) else 0\n",
    "    )    \n",
    "    for index, row in input_df.iterrows():\n",
    "        if row['is_dna_binding'] == 1:\n",
    "            label_array[row['start']:row['end'] + 1] = 1\n",
    "    \n",
    "    display(input_df)\n",
    "    \n",
    "    return label_array\n",
    "\n",
    "# Example usage\n",
    "# test_seq= \"MGRGKVKPNRKSTGDNSNVVTMIRAGSYPKVNPTPTWVRAIPFEVSVQSGIAFKVPVGSLFSANFRTDSFTSVTVMSVRAWTQLTPPVNEYSFVRLKPLFKTGDSTEEFEGRASNINTRASVGYRIPTNLRQNTVAADNVCEVRSNCRQVALVISCCFN\"\n",
    "# test_seq = \"MAVHLLIVDALNLIRRIHAVQGSPCVETCQHALDQLIMHSQPTHAVAVFDDENRSSGWRHQRLPDYKAGRPPMPEELHDEMPALRAAFEQRGVPCWSTSGNEADDLAATLAVKVTQAGHQATIVSTDKGYCQLLSPTLRIRDYFQKRWLDAPFIDKEFGVQPQQLPDYWGLAGISSSKVPGVAGIGPKSATQLLVEFQSLEGIYENLDAVAEKWRKKLETHKEMAFLCRDIARLQTDLHIDGNLQQLRLVR\" # Go_terms none\n",
    "\n",
    "# output_file = run_hmmscan(test_seq)\n",
    "# parsed_df = parse_hmmscan_output(output_file)\n",
    "# parsed_df = add_go_terms(parsed_df)\n",
    "# dna_binding_labels = label_dna_binding(parsed_df, len(test_seq))  # Example GO terms\n",
    "# print(dna_binding_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-generate domain annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sequences(input_csv, output_file):\n",
    "    # The fasta file is a list of sequences \n",
    "    # Open the fasta file, processs them sequence by sequence\n",
    "    list_of_domain_annotations = list()\n",
    "    \n",
    "    df = pd.read_csv(input_csv)\n",
    "    for seq in df['seq']:\n",
    "        run_hmmscan(seq)\n",
    "        pfam_df = add_go_terms(parse_hmmscan_output(\"r_d.out\"))\n",
    "        res = label_dna_binding(pfam_df, len(seq))\n",
    "        print(res)\n",
    "        list_of_domain_annotations.append(res)\n",
    "    \n",
    "    # list_of_domain_annotations = np.array(list_of_domain_annotations)\n",
    "    # np.save(\"dom_annotations_test.npy\", list_of_domain_annotations)\n",
    "    print(len(list_of_domain_annotations))\n",
    "    pickle.dump(list_of_domain_annotations, open(output_file, \"wb\"))\n",
    "    return list_of_domain_annotations\n",
    "\n",
    "# process_sequences(\"DRNA_TRAIN.csv\", \"dom_annotations_train.pkl\")\n",
    "# process_sequences(\"DRNA_TEST.csv\", \"dom_annotations_test.pkl\")\n",
    "\n",
    "def get_domain_array_from_file(file_name):\n",
    "    return np.load(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D matrix to feature vector using CNN on PSSM, using autoencoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.04796880475343637\n",
      "Epoch 2, Loss: 0.03350876864481343\n",
      "Epoch 3, Loss: 0.03260732287590734\n",
      "Epoch 4, Loss: 0.032055714290726366\n",
      "Epoch 5, Loss: 0.03159040979463536\n"
     ]
    }
   ],
   "source": [
    "class SimplePSSMAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimplePSSMAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),  # output: [16, 5, 10]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # output: [32, 3, 5]\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),  # flatten the feature maps\n",
    "            nn.Linear(480, 32)  # down to 32 features\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 480),\n",
    "            nn.Unflatten(1, (32, 3, 5)),  # reshape to [32, 3, 5]\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=(0, 1)),  # output: [16, 5, 10]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=(0, 1)),  # output: [1, 9, 20]\n",
    "            nn.Sigmoid()  # use sigmoid to scale the output between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Initialize the model, criterion, and optimizer\n",
    "pssm_encoder = SimplePSSMAutoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(pssm_encoder.parameters(), lr=1e-3)\n",
    "\n",
    "def train_autoencoder():\n",
    "    # Load and preprocess data\n",
    "    with open(\"generated_pssms_train.pkl\", 'rb') as file:\n",
    "        full_pssm = pickle.load(file)\n",
    "\n",
    "    full_sliced_pssm_list = [get_all_slices(rescale_pssm(pssm), 9) for pssm in full_pssm if pssm is not None]\n",
    "    full_sliced_pssm_list = [slice for sublist in full_sliced_pssm_list for slice in sublist]\n",
    "\n",
    "    pssm_tensor = torch.tensor(full_sliced_pssm_list, dtype=torch.float32).unsqueeze(1)\n",
    "    # print(pssm_tensor.shape)\n",
    "\n",
    "    dataset = TensorDataset(pssm_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in loader:\n",
    "            pssm = batch[0]  # unpack the batch\n",
    "            decoded = pssm_encoder(pssm)\n",
    "            loss = criterion(decoded, pssm)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(loader)}\")\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencode_pssm(input_pssm : np.ndarray):\n",
    "    input_tensor = torch.tensor(input_pssm, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    decoded, encoded = pssm_encoder(input_tensor)\n",
    "    return decoded.squeeze().detach().numpy()\n",
    "\n",
    "# autoencode_pssm(np.random.rand(9, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming these functions are correctly implemented\n",
    "def get_all_features_for_one_sequence(full_seq: str, dna_label: str, input_pssm : np.ndarray, seq_diso_values, domain_values, input_hhm : np.ndarray, window_size: int = 9) -> list:\n",
    "    seq_list = get_instances_from_seq(full_seq, window_size=window_size)  # Assuming this returns a list of sequences of length window_size\n",
    "    pssm = rescale_pssm(input_pssm)  # Assuming this returns a PSSM for the full_seq\n",
    "    # pssm = np.array(input_pssm)[:, :20]\n",
    "    # print(pssm.shape)\n",
    "\n",
    "    all_features_list = []  # Use a list to maintain structure\n",
    "\n",
    "    for index, seq in enumerate(seq_list):\n",
    "        # print(f\"Processing sequence {index} of {len(seq_list)}\")\n",
    "        current_residue_label = dna_label[index + window_size // 2]\n",
    "        if current_residue_label == '2':\n",
    "            # print(f\"Residue unknown at index {index}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        pssm_pp_features = create_pssm_pp(get_sliced_pssm(pssm, index, window_size).T, PP_MATRIX).flatten()\n",
    "        # pssm = get_sliced_pssm(input_pssm, index, window_size)\n",
    "        pssm_32d = autoencode_pssm(get_sliced_pssm(input_pssm, index, window_size)).flatten()\n",
    "        aac_features = np.append(calculate_AAC_PC(seq), calculate_AAC_H(seq))\n",
    "        obv_features = get_full_obv(seq) \n",
    "        diso_features = [0 if x < 0.5 else 1 for x in seq_diso_values[index:index+window_size]]\n",
    "        domain_features = domain_values[index:index+window_size]\n",
    "        hhm_features = np.array(input_hhm[index:index+window_size]).flatten()\n",
    "        \n",
    "        # all_features = np.concatenate([pssm_pp_features, aac_features, obv_features, domain_features])\n",
    "        # all_features = np.concatenate([aac_features, obv_features, diso_features, domain_features, hhm_features])\n",
    "        # all_features = np.concatenate([aac_features, obv_features, diso_features, domain_features, pssm_32d])\n",
    "        all_features = np.concatenate([aac_features, obv_features, domain_features, pssm_pp_features])\n",
    "        all_features_list.append((all_features, current_residue_label))\n",
    "\n",
    "    return all_features_list\n",
    "\n",
    "# Generate feature vectors for each sequence in the training dataset\n",
    "def get_all_features_for_dataset(dataset: pd.DataFrame, generated_pssm_file, generated_diso_file, generated_domain_file, generated_hhm_file, window_size : int = 9) -> list:\n",
    "    full_pssm = list(pickle.load(open(generated_pssm_file, 'rb'))) \n",
    "    full_diso_values = list(pickle.load(open(generated_diso_file, 'rb')))   \n",
    "    full_domain_values = list(pickle.load(open(generated_domain_file, 'rb')))  \n",
    "    full_hhm_values = list(pickle.load(open(generated_hhm_file, 'rb')))\n",
    "    \n",
    "    all_features_list = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        if full_pssm[index] is None:\n",
    "            print(f\"Skipping sequence at index {index} due to missing PSSM\")\n",
    "            continue\n",
    "        if full_diso_values[index] is None:\n",
    "            print(f\"Diso value not available! Index: {index}\")\n",
    "            continue\n",
    "        # if full_domain_values[index] is None:\n",
    "        #     print(f\"Domain value not available! Index: {index}\")\n",
    "        #     continue\n",
    "        try:\n",
    "            all_features_list.extend(get_all_features_for_one_sequence(full_seq=row['seq'], \n",
    "                                                                       dna_label=row['dna_label'],\n",
    "                                                                       input_pssm = full_pssm[index], \n",
    "                                                                       seq_diso_values=full_diso_values[index],\n",
    "                                                                       domain_values=full_domain_values[index],\n",
    "                                                                       input_hhm=full_hhm_values[index],\n",
    "                                                                       window_size=window_size))\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error processing sequence at index {index}: {e}\")\n",
    "            continue\n",
    "    return all_features_list\n",
    "\n",
    "# %load_ext line_profiler\n",
    "# %lprun -f get_all_features_for_one_sequence get_all_features_for_one_sequence(\"MKIAIINMGNNVINFKTVPSSETIYLFKVISEMGLNVDIISLKNGVYTKSFDEVDVNDYDRLIVVNSSINFFGGKPNLAILSAQKFMAKYKSKIYYLFTDIRLPFSQSWPNVKNRPWAYLYTEEELLIKSPIKVISQGINLDIAKAAHKKVDNVIEFEYFPIEQYKIHMNDFQLSKPTKKTLDVIYGGSFRSGQRESKMVEFLFDTGLNIEFFGNAREKQFKNPKYPWTKAPVFTGKIPMNMVSEKNSQAIAALIIGDKNYNDNFITLRVWETMASDAVMLIDEEFDTKHRIINDARFYVNNRAELIDRVNELKHSDVLRKEMLSIQHDILNKTRAKKAEWQDAFKKAID\",\"000000000000110111000100000000000000000000000000000000000000000000010111111100000000000000000000000001000000000000111010000000000000000000000000000011000000000000000000000000000010000000001001111111000000000000000010101100001000000000011000000000000000000000100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping sequence at index 119 due to missing PSSM\n",
      "Skipping sequence at index 179 due to missing PSSM\n",
      "Skipping sequence at index 211 due to missing PSSM\n",
      "Skipping sequence at index 234 due to missing PSSM\n",
      "Skipping sequence at index 264 due to missing PSSM\n",
      "Skipping sequence at index 268 due to missing PSSM\n",
      "Skipping sequence at index 293 due to missing PSSM\n",
      "Skipping sequence at index 349 due to missing PSSM\n",
      "Skipping sequence at index 357 due to missing PSSM\n",
      "Skipping sequence at index 368 due to missing PSSM\n",
      "Skipping sequence at index 387 due to missing PSSM\n",
      "Skipping sequence at index 391 due to missing PSSM\n",
      "Skipping sequence at index 403 due to missing PSSM\n",
      "Skipping sequence at index 405 due to missing PSSM\n",
      "Skipping sequence at index 410 due to missing PSSM\n",
      "Skipping sequence at index 422 due to missing PSSM\n",
      "Skipping sequence at index 436 due to missing PSSM\n",
      "Skipping sequence at index 439 due to missing PSSM\n",
      "Skipping sequence at index 452 due to missing PSSM\n",
      "Skipping sequence at index 5 due to missing PSSM\n",
      "Skipping sequence at index 16 due to missing PSSM\n",
      "Skipping sequence at index 51 due to missing PSSM\n",
      "Skipping sequence at index 58 due to missing PSSM\n",
      "Skipping sequence at index 66 due to missing PSSM\n",
      "X_train shape: (96548, 134)\n",
      "y_train shape: (96548,)\n",
      "X_test shape: (17952, 134)\n",
      "y_test shape: (17952,)\n"
     ]
    }
   ],
   "source": [
    "# Assuming training_dataset is loaded correctly\n",
    "training_dataset = pd.read_csv(\"DRNA_TRAIN.csv\")\n",
    "test_dataset = pd.read_csv(\"DRNA_TEST.csv\")\n",
    "\n",
    "# all_training_features = get_all_features_for_dataset(training_dataset, \"generated_pssms_train.pkl\", \"disorder_preds_train.pkl\", \"dom_annotations_train.pkl\", window_size=20)\n",
    "# all_test_features = get_all_features_for_dataset(test_dataset, \"generated_pssms_test.pkl\", \"disorder_preds_test.pkl\", \"dom_annotations_test.pkl\", window_size=20)    \n",
    "all_training_features = get_all_features_for_dataset(training_dataset, \"generated_pssms_train.pkl\", \"disorder_preds_train.pkl\", \"dom_annotations_train.pkl\", \"generated_hhm_train.pkl\")\n",
    "all_test_features = get_all_features_for_dataset(test_dataset, \"generated_pssms_test.pkl\", \"disorder_preds_test.pkl\", \"dom_annotations_test.pkl\", \"generated_hhm_test.pkl\")\n",
    "\n",
    "\n",
    "# Separate into X_train and y_train\n",
    "X_train = [features for features, label in all_training_features]\n",
    "y_train = [label for features, label in all_training_features]\n",
    "X_test = [features for features, label in all_test_features]\n",
    "y_test = [label for features, label in all_test_features]\n",
    "\n",
    "# Optionally convert to numpy arrays for compatibility with scikit-learn\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Compilation of results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample X_train and y_train such that they contain equal amounts of positive and negative samples\n",
    "# Assuming y_train contains binary labels where 1 is positive and 0 is negative\n",
    "\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: purple' if v else '' for v in is_max]\n",
    "\n",
    "def shuffle(random_state=None):\n",
    "    positive_indices = np.where(y_train == '1')[0]\n",
    "    negative_indices = np.where(y_train == '0')[0]\n",
    "\n",
    "\n",
    "    # # Determine the number of samples to subsample based on the smaller class\n",
    "    n_samples = min(positive_indices.shape[0], negative_indices.shape[0]) \n",
    "\n",
    "    # Randomly select n_samples from both positive and negative indices\n",
    "    positive_subsample_indices = np.random.choice(positive_indices, n_samples, replace=False)\n",
    "    negative_subsample_indices = np.random.choice(negative_indices, n_samples, replace=False)\n",
    "    # unknkown_subsample_indices = np.random.choice(unknown_indices, n_samples, replace=True)\n",
    "\n",
    "    # Concatenate the subsampled indices and then use them to create subsampled X_train and y_train\n",
    "    subsample_indices = np.concatenate([positive_subsample_indices, negative_subsample_indices])\n",
    "    X_train_subsampled = X_train[subsample_indices]\n",
    "    y_train_subsampled = y_train[subsample_indices]\n",
    "    print(X_train_subsampled.shape, y_train_subsampled.shape)\n",
    "\n",
    "    shuffle_indices = np.random.permutation(len(X_train_subsampled))\n",
    "    X_train_subsampled = X_train_subsampled[shuffle_indices]\n",
    "    y_train_subsampled = y_train_subsampled[shuffle_indices]\n",
    "    \n",
    "    return X_train_subsampled, y_train_subsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14222, 134) (14222,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_bae44_row24_col1 {\n",
       "  background-color: purple;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_bae44\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_bae44_level0_col0\" class=\"col_heading level0 col0\" >Threshold</th>\n",
       "      <th id=\"T_bae44_level0_col1\" class=\"col_heading level0 col1\" >MCC</th>\n",
       "      <th id=\"T_bae44_level0_col2\" class=\"col_heading level0 col2\" >Sensitivity</th>\n",
       "      <th id=\"T_bae44_level0_col3\" class=\"col_heading level0 col3\" >Specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_bae44_row0_col0\" class=\"data row0 col0\" >0.10</td>\n",
       "      <td id=\"T_bae44_row0_col1\" class=\"data row0 col1\" >0.01</td>\n",
       "      <td id=\"T_bae44_row0_col2\" class=\"data row0 col2\" >1.00</td>\n",
       "      <td id=\"T_bae44_row0_col3\" class=\"data row0 col3\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_bae44_row1_col0\" class=\"data row1 col0\" >0.12</td>\n",
       "      <td id=\"T_bae44_row1_col1\" class=\"data row1 col1\" >0.02</td>\n",
       "      <td id=\"T_bae44_row1_col2\" class=\"data row1 col2\" >1.00</td>\n",
       "      <td id=\"T_bae44_row1_col3\" class=\"data row1 col3\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_bae44_row2_col0\" class=\"data row2 col0\" >0.14</td>\n",
       "      <td id=\"T_bae44_row2_col1\" class=\"data row2 col1\" >0.02</td>\n",
       "      <td id=\"T_bae44_row2_col2\" class=\"data row2 col2\" >1.00</td>\n",
       "      <td id=\"T_bae44_row2_col3\" class=\"data row2 col3\" >0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_bae44_row3_col0\" class=\"data row3 col0\" >0.16</td>\n",
       "      <td id=\"T_bae44_row3_col1\" class=\"data row3 col1\" >0.04</td>\n",
       "      <td id=\"T_bae44_row3_col2\" class=\"data row3 col2\" >0.99</td>\n",
       "      <td id=\"T_bae44_row3_col3\" class=\"data row3 col3\" >0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_bae44_row4_col0\" class=\"data row4 col0\" >0.18</td>\n",
       "      <td id=\"T_bae44_row4_col1\" class=\"data row4 col1\" >0.05</td>\n",
       "      <td id=\"T_bae44_row4_col2\" class=\"data row4 col2\" >0.98</td>\n",
       "      <td id=\"T_bae44_row4_col3\" class=\"data row4 col3\" >0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_bae44_row5_col0\" class=\"data row5 col0\" >0.20</td>\n",
       "      <td id=\"T_bae44_row5_col1\" class=\"data row5 col1\" >0.05</td>\n",
       "      <td id=\"T_bae44_row5_col2\" class=\"data row5 col2\" >0.98</td>\n",
       "      <td id=\"T_bae44_row5_col3\" class=\"data row5 col3\" >0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_bae44_row6_col0\" class=\"data row6 col0\" >0.22</td>\n",
       "      <td id=\"T_bae44_row6_col1\" class=\"data row6 col1\" >0.07</td>\n",
       "      <td id=\"T_bae44_row6_col2\" class=\"data row6 col2\" >0.96</td>\n",
       "      <td id=\"T_bae44_row6_col3\" class=\"data row6 col3\" >0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_bae44_row7_col0\" class=\"data row7 col0\" >0.24</td>\n",
       "      <td id=\"T_bae44_row7_col1\" class=\"data row7 col1\" >0.08</td>\n",
       "      <td id=\"T_bae44_row7_col2\" class=\"data row7 col2\" >0.95</td>\n",
       "      <td id=\"T_bae44_row7_col3\" class=\"data row7 col3\" >0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_bae44_row8_col0\" class=\"data row8 col0\" >0.26</td>\n",
       "      <td id=\"T_bae44_row8_col1\" class=\"data row8 col1\" >0.09</td>\n",
       "      <td id=\"T_bae44_row8_col2\" class=\"data row8 col2\" >0.94</td>\n",
       "      <td id=\"T_bae44_row8_col3\" class=\"data row8 col3\" >0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_bae44_row9_col0\" class=\"data row9 col0\" >0.28</td>\n",
       "      <td id=\"T_bae44_row9_col1\" class=\"data row9 col1\" >0.10</td>\n",
       "      <td id=\"T_bae44_row9_col2\" class=\"data row9 col2\" >0.93</td>\n",
       "      <td id=\"T_bae44_row9_col3\" class=\"data row9 col3\" >0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_bae44_row10_col0\" class=\"data row10 col0\" >0.30</td>\n",
       "      <td id=\"T_bae44_row10_col1\" class=\"data row10 col1\" >0.12</td>\n",
       "      <td id=\"T_bae44_row10_col2\" class=\"data row10 col2\" >0.90</td>\n",
       "      <td id=\"T_bae44_row10_col3\" class=\"data row10 col3\" >0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_bae44_row11_col0\" class=\"data row11 col0\" >0.32</td>\n",
       "      <td id=\"T_bae44_row11_col1\" class=\"data row11 col1\" >0.13</td>\n",
       "      <td id=\"T_bae44_row11_col2\" class=\"data row11 col2\" >0.87</td>\n",
       "      <td id=\"T_bae44_row11_col3\" class=\"data row11 col3\" >0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_bae44_row12_col0\" class=\"data row12 col0\" >0.34</td>\n",
       "      <td id=\"T_bae44_row12_col1\" class=\"data row12 col1\" >0.14</td>\n",
       "      <td id=\"T_bae44_row12_col2\" class=\"data row12 col2\" >0.84</td>\n",
       "      <td id=\"T_bae44_row12_col3\" class=\"data row12 col3\" >0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_bae44_row13_col0\" class=\"data row13 col0\" >0.36</td>\n",
       "      <td id=\"T_bae44_row13_col1\" class=\"data row13 col1\" >0.16</td>\n",
       "      <td id=\"T_bae44_row13_col2\" class=\"data row13 col2\" >0.82</td>\n",
       "      <td id=\"T_bae44_row13_col3\" class=\"data row13 col3\" >0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_bae44_row14_col0\" class=\"data row14 col0\" >0.38</td>\n",
       "      <td id=\"T_bae44_row14_col1\" class=\"data row14 col1\" >0.16</td>\n",
       "      <td id=\"T_bae44_row14_col2\" class=\"data row14 col2\" >0.81</td>\n",
       "      <td id=\"T_bae44_row14_col3\" class=\"data row14 col3\" >0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_bae44_row15_col0\" class=\"data row15 col0\" >0.40</td>\n",
       "      <td id=\"T_bae44_row15_col1\" class=\"data row15 col1\" >0.17</td>\n",
       "      <td id=\"T_bae44_row15_col2\" class=\"data row15 col2\" >0.77</td>\n",
       "      <td id=\"T_bae44_row15_col3\" class=\"data row15 col3\" >0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_bae44_row16_col0\" class=\"data row16 col0\" >0.42</td>\n",
       "      <td id=\"T_bae44_row16_col1\" class=\"data row16 col1\" >0.19</td>\n",
       "      <td id=\"T_bae44_row16_col2\" class=\"data row16 col2\" >0.71</td>\n",
       "      <td id=\"T_bae44_row16_col3\" class=\"data row16 col3\" >0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_bae44_row17_col0\" class=\"data row17 col0\" >0.44</td>\n",
       "      <td id=\"T_bae44_row17_col1\" class=\"data row17 col1\" >0.20</td>\n",
       "      <td id=\"T_bae44_row17_col2\" class=\"data row17 col2\" >0.67</td>\n",
       "      <td id=\"T_bae44_row17_col3\" class=\"data row17 col3\" >0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_bae44_row18_col0\" class=\"data row18 col0\" >0.46</td>\n",
       "      <td id=\"T_bae44_row18_col1\" class=\"data row18 col1\" >0.20</td>\n",
       "      <td id=\"T_bae44_row18_col2\" class=\"data row18 col2\" >0.63</td>\n",
       "      <td id=\"T_bae44_row18_col3\" class=\"data row18 col3\" >0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_bae44_row19_col0\" class=\"data row19 col0\" >0.48</td>\n",
       "      <td id=\"T_bae44_row19_col1\" class=\"data row19 col1\" >0.20</td>\n",
       "      <td id=\"T_bae44_row19_col2\" class=\"data row19 col2\" >0.58</td>\n",
       "      <td id=\"T_bae44_row19_col3\" class=\"data row19 col3\" >0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_bae44_row20_col0\" class=\"data row20 col0\" >0.50</td>\n",
       "      <td id=\"T_bae44_row20_col1\" class=\"data row20 col1\" >0.21</td>\n",
       "      <td id=\"T_bae44_row20_col2\" class=\"data row20 col2\" >0.54</td>\n",
       "      <td id=\"T_bae44_row20_col3\" class=\"data row20 col3\" >0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_bae44_row21_col0\" class=\"data row21 col0\" >0.52</td>\n",
       "      <td id=\"T_bae44_row21_col1\" class=\"data row21 col1\" >0.21</td>\n",
       "      <td id=\"T_bae44_row21_col2\" class=\"data row21 col2\" >0.49</td>\n",
       "      <td id=\"T_bae44_row21_col3\" class=\"data row21 col3\" >0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_bae44_row22_col0\" class=\"data row22 col0\" >0.54</td>\n",
       "      <td id=\"T_bae44_row22_col1\" class=\"data row22 col1\" >0.21</td>\n",
       "      <td id=\"T_bae44_row22_col2\" class=\"data row22 col2\" >0.46</td>\n",
       "      <td id=\"T_bae44_row22_col3\" class=\"data row22 col3\" >0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_bae44_row23_col0\" class=\"data row23 col0\" >0.56</td>\n",
       "      <td id=\"T_bae44_row23_col1\" class=\"data row23 col1\" >0.21</td>\n",
       "      <td id=\"T_bae44_row23_col2\" class=\"data row23 col2\" >0.42</td>\n",
       "      <td id=\"T_bae44_row23_col3\" class=\"data row23 col3\" >0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_bae44_row24_col0\" class=\"data row24 col0\" >0.58</td>\n",
       "      <td id=\"T_bae44_row24_col1\" class=\"data row24 col1\" >0.22</td>\n",
       "      <td id=\"T_bae44_row24_col2\" class=\"data row24 col2\" >0.37</td>\n",
       "      <td id=\"T_bae44_row24_col3\" class=\"data row24 col3\" >0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_bae44_row25_col0\" class=\"data row25 col0\" >0.60</td>\n",
       "      <td id=\"T_bae44_row25_col1\" class=\"data row25 col1\" >0.21</td>\n",
       "      <td id=\"T_bae44_row25_col2\" class=\"data row25 col2\" >0.32</td>\n",
       "      <td id=\"T_bae44_row25_col3\" class=\"data row25 col3\" >0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_bae44_row26_col0\" class=\"data row26 col0\" >0.62</td>\n",
       "      <td id=\"T_bae44_row26_col1\" class=\"data row26 col1\" >0.20</td>\n",
       "      <td id=\"T_bae44_row26_col2\" class=\"data row26 col2\" >0.27</td>\n",
       "      <td id=\"T_bae44_row26_col3\" class=\"data row26 col3\" >0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_bae44_row27_col0\" class=\"data row27 col0\" >0.64</td>\n",
       "      <td id=\"T_bae44_row27_col1\" class=\"data row27 col1\" >0.20</td>\n",
       "      <td id=\"T_bae44_row27_col2\" class=\"data row27 col2\" >0.24</td>\n",
       "      <td id=\"T_bae44_row27_col3\" class=\"data row27 col3\" >0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_bae44_row28_col0\" class=\"data row28 col0\" >0.66</td>\n",
       "      <td id=\"T_bae44_row28_col1\" class=\"data row28 col1\" >0.19</td>\n",
       "      <td id=\"T_bae44_row28_col2\" class=\"data row28 col2\" >0.22</td>\n",
       "      <td id=\"T_bae44_row28_col3\" class=\"data row28 col3\" >0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_bae44_row29_col0\" class=\"data row29 col0\" >0.68</td>\n",
       "      <td id=\"T_bae44_row29_col1\" class=\"data row29 col1\" >0.18</td>\n",
       "      <td id=\"T_bae44_row29_col2\" class=\"data row29 col2\" >0.19</td>\n",
       "      <td id=\"T_bae44_row29_col3\" class=\"data row29 col3\" >0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "      <td id=\"T_bae44_row30_col0\" class=\"data row30 col0\" >0.70</td>\n",
       "      <td id=\"T_bae44_row30_col1\" class=\"data row30 col1\" >0.18</td>\n",
       "      <td id=\"T_bae44_row30_col2\" class=\"data row30 col2\" >0.16</td>\n",
       "      <td id=\"T_bae44_row30_col3\" class=\"data row30 col3\" >0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "      <td id=\"T_bae44_row31_col0\" class=\"data row31 col0\" >0.72</td>\n",
       "      <td id=\"T_bae44_row31_col1\" class=\"data row31 col1\" >0.17</td>\n",
       "      <td id=\"T_bae44_row31_col2\" class=\"data row31 col2\" >0.14</td>\n",
       "      <td id=\"T_bae44_row31_col3\" class=\"data row31 col3\" >0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "      <td id=\"T_bae44_row32_col0\" class=\"data row32 col0\" >0.74</td>\n",
       "      <td id=\"T_bae44_row32_col1\" class=\"data row32 col1\" >0.18</td>\n",
       "      <td id=\"T_bae44_row32_col2\" class=\"data row32 col2\" >0.12</td>\n",
       "      <td id=\"T_bae44_row32_col3\" class=\"data row32 col3\" >0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
       "      <td id=\"T_bae44_row33_col0\" class=\"data row33 col0\" >0.76</td>\n",
       "      <td id=\"T_bae44_row33_col1\" class=\"data row33 col1\" >0.17</td>\n",
       "      <td id=\"T_bae44_row33_col2\" class=\"data row33 col2\" >0.10</td>\n",
       "      <td id=\"T_bae44_row33_col3\" class=\"data row33 col3\" >0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
       "      <td id=\"T_bae44_row34_col0\" class=\"data row34 col0\" >0.78</td>\n",
       "      <td id=\"T_bae44_row34_col1\" class=\"data row34 col1\" >0.16</td>\n",
       "      <td id=\"T_bae44_row34_col2\" class=\"data row34 col2\" >0.08</td>\n",
       "      <td id=\"T_bae44_row34_col3\" class=\"data row34 col3\" >0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
       "      <td id=\"T_bae44_row35_col0\" class=\"data row35 col0\" >0.80</td>\n",
       "      <td id=\"T_bae44_row35_col1\" class=\"data row35 col1\" >0.15</td>\n",
       "      <td id=\"T_bae44_row35_col2\" class=\"data row35 col2\" >0.07</td>\n",
       "      <td id=\"T_bae44_row35_col3\" class=\"data row35 col3\" >0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
       "      <td id=\"T_bae44_row36_col0\" class=\"data row36 col0\" >0.82</td>\n",
       "      <td id=\"T_bae44_row36_col1\" class=\"data row36 col1\" >0.14</td>\n",
       "      <td id=\"T_bae44_row36_col2\" class=\"data row36 col2\" >0.06</td>\n",
       "      <td id=\"T_bae44_row36_col3\" class=\"data row36 col3\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
       "      <td id=\"T_bae44_row37_col0\" class=\"data row37 col0\" >0.84</td>\n",
       "      <td id=\"T_bae44_row37_col1\" class=\"data row37 col1\" >0.12</td>\n",
       "      <td id=\"T_bae44_row37_col2\" class=\"data row37 col2\" >0.04</td>\n",
       "      <td id=\"T_bae44_row37_col3\" class=\"data row37 col3\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
       "      <td id=\"T_bae44_row38_col0\" class=\"data row38 col0\" >0.86</td>\n",
       "      <td id=\"T_bae44_row38_col1\" class=\"data row38 col1\" >0.10</td>\n",
       "      <td id=\"T_bae44_row38_col2\" class=\"data row38 col2\" >0.03</td>\n",
       "      <td id=\"T_bae44_row38_col3\" class=\"data row38 col3\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
       "      <td id=\"T_bae44_row39_col0\" class=\"data row39 col0\" >0.88</td>\n",
       "      <td id=\"T_bae44_row39_col1\" class=\"data row39 col1\" >0.08</td>\n",
       "      <td id=\"T_bae44_row39_col2\" class=\"data row39 col2\" >0.02</td>\n",
       "      <td id=\"T_bae44_row39_col3\" class=\"data row39 col3\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
       "      <td id=\"T_bae44_row40_col0\" class=\"data row40 col0\" >0.90</td>\n",
       "      <td id=\"T_bae44_row40_col1\" class=\"data row40 col1\" >0.07</td>\n",
       "      <td id=\"T_bae44_row40_col2\" class=\"data row40 col2\" >0.01</td>\n",
       "      <td id=\"T_bae44_row40_col3\" class=\"data row40 col3\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
       "      <td id=\"T_bae44_row41_col0\" class=\"data row41 col0\" >0.92</td>\n",
       "      <td id=\"T_bae44_row41_col1\" class=\"data row41 col1\" >0.05</td>\n",
       "      <td id=\"T_bae44_row41_col2\" class=\"data row41 col2\" >0.01</td>\n",
       "      <td id=\"T_bae44_row41_col3\" class=\"data row41 col3\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bae44_level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
       "      <td id=\"T_bae44_row42_col0\" class=\"data row42 col0\" >0.94</td>\n",
       "      <td id=\"T_bae44_row42_col1\" class=\"data row42 col1\" >0.01</td>\n",
       "      <td id=\"T_bae44_row42_col2\" class=\"data row42 col2\" >0.00</td>\n",
       "      <td id=\"T_bae44_row42_col3\" class=\"data row42 col3\" >1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f0550ae6740>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "X_train_subsampled, y_train_subsampled = shuffle()\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_subsampled, y_train_subsampled)\n",
    "\n",
    "# Get prediction probabilities\n",
    "predictions = rf_model.predict_proba(X_test)\n",
    "results = []\n",
    "\n",
    "# Iterate over thresholds from 0.1 to 0.96 with a step of 0.02\n",
    "for threshold in np.arange(0.1, 0.96, 0.02):\n",
    "    y_pred = ['1' if p[1] >= threshold else '0' for p in predictions]\n",
    "    \n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=['0', '1']).ravel()\n",
    "    \n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    \n",
    "    results.append((threshold, mcc, sensitivity, specificity))\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Threshold', 'MCC', 'Sensitivity', 'Specificity'])\n",
    "results_df = results_df.round(2)\n",
    "styled_results_df = results_df.style.format(\"{:.2f}\").apply(highlight_max, subset=['MCC'])\n",
    "display(styled_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "best_results = []\n",
    "\n",
    "# Iterate over 10 different random seeds\n",
    "for seed in range(10):\n",
    "    X_train_subsampled, y_train_subsampled = shuffle() \n",
    "    rf_model = RandomForestClassifier()\n",
    "    rf_model.fit(X_train_subsampled, y_train_subsampled)\n",
    "\n",
    "    # Get prediction probabilities\n",
    "    predictions = rf_model.predict_proba(X_test)\n",
    "\n",
    "    # Initialize the best values tracker\n",
    "    best_mcc, best_sen, best_spe = 0, 0, 0\n",
    "    best_threshold = 0\n",
    "\n",
    "    # Iterate over thresholds from 0.1 to 0.96 with a step of 0.02\n",
    "    for threshold in np.arange(0.1, 0.96, 0.02):\n",
    "        y_pred = ['1' if p[1] >= threshold else '0' for p in predictions]\n",
    "        \n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=['0', '1']).ravel()\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "\n",
    "        # Store all results\n",
    "        results.append((seed, threshold, mcc, sensitivity, specificity))\n",
    "\n",
    "        # Update best values if current MCC is the highest found so far\n",
    "        if mcc > best_mcc:\n",
    "            best_mcc = mcc\n",
    "            best_sen = sensitivity\n",
    "            best_spe = specificity\n",
    "            best_threshold = threshold\n",
    "\n",
    "    # Record the best values for the current seed\n",
    "    best_results.append((seed, best_threshold, best_mcc, best_sen, best_spe))\n",
    "\n",
    "# Optionally, convert the results list to a DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results, columns=['Seed', 'Threshold', 'MCC', 'Sensitivity', 'Specificity'])\n",
    "best_results_df = pd.DataFrame(best_results, columns=['Seed', 'Best Threshold', 'Best MCC', 'Best Sensitivity', 'Best Specificity'])\n",
    "\n",
    "print(\"Detailed Results:\")\n",
    "print(results_df.head())  # Modify as needed to display more or specific data\n",
    "print(\"\\nBest Results Per Seed:\")\n",
    "print(best_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# rf_model = RandomForestClassifier(random_state=42)\n",
    "# rf_model.fit(X_train_subsampled, y_train_subsampled)\n",
    "\n",
    "threshold = 0.42\n",
    "\n",
    "# Get prediction probabilities\n",
    "predictions = rf_model.predict_proba(X_test)\n",
    "y_pred = [1 if p[1] >= threshold else 0 for p in predictions]\n",
    "# print(y_pred)\n",
    "\n",
    "phi_threshold = 0.45\n",
    "scaling_const = 40\n",
    "\n",
    "# d_score = np.abs(predictions[:, 1] - phi_threshold)\n",
    "d_score = np.abs(predictions[:, 1] - threshold)\n",
    "# print(d_score)\n",
    "reliability_index = np.clip(scaling_const * (d_score), -1, 10)\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# get all correctly predicted and incorrectly predicted positive indices\n",
    "correct_positive_indices = np.where((y_test == '1') & (y_pred == 1))[0]\n",
    "incorrect_positive_indices = np.where((y_test == '1') & (y_pred == 0))[0]\n",
    "print(correct_positive_indices.shape, incorrect_positive_indices.shape)\n",
    "\n",
    "correct_positive_ri = reliability_index[correct_positive_indices]\n",
    "incorrect_positive_ri = reliability_index[incorrect_positive_indices]\n",
    "\n",
    "correct_negative_indices = np.where((y_test == '0') & (y_pred == 0))[0]\n",
    "incorrect_negative_indices = np.where((y_test == '0') & (y_pred == 1))[0]\n",
    "print(correct_negative_indices.shape, incorrect_negative_indices.shape)\n",
    "\n",
    "correct_negative_ri = reliability_index[correct_negative_indices]\n",
    "incorrect_negative_ri = reliability_index[incorrect_negative_indices]\n",
    "\n",
    "# Do a boxplot of the reliability index for correctly predicted and incorrectly predicted positive samples\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([correct_positive_ri, incorrect_positive_ri, correct_negative_ri, incorrect_negative_ri], labels=['TP', 'FP', 'TN', 'FN'])\n",
    "plt.xlabel('Prediction Outcome')\n",
    "plt.ylabel('Reliability Index')\n",
    "plt.title('Reliability Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Plot a graph of accuracy against reliability index\n",
    "reliability_index = np.round(np.array(reliability_index))\n",
    "accuracies = []\n",
    "ri_sample_percentages = []\n",
    "\n",
    "for ri in range(0, 11):\n",
    "    # get all indices in reliability index with score ri\n",
    "    ri_indices = np.where(reliability_index == ri)[0]\n",
    "    # get the predictions for these indices\n",
    "    ri_predictions = np.array(y_pred[ri_indices]).astype(int)\n",
    "    # get the actual labels for these indices\n",
    "    ri_actual = np.array(y_test[ri_indices]).astype(int)\n",
    "\n",
    "    ri_sample_percentages.append(np.round(ri_indices.shape[0] / y_test.shape[0] * 100, 1))\n",
    "    \n",
    "    # calculate the accuracy for these indices\n",
    "    accuracy = accuracy_score(ri_actual, ri_predictions)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "accuracies = np.round(np.array(accuracies), 2)\n",
    "print(accuracies)\n",
    "print(ri_sample_percentages)\n",
    "\n",
    "# for ri in range(0, 11):\n",
    "#     # get all indices in reliability index with score ri\n",
    "#     ri_indices = np.where((reliability_index == ri) & (y_test == '1'))[0]\n",
    "#     # get the predictions for these indices\n",
    "#     ri_predictions = np.array(y_pred[ri_indices]).astype(int)\n",
    "#     # get the actual labels for these indices\n",
    "#     ri_actual = np.array(y_test[ri_indices]).astype(int)\n",
    "\n",
    "#     ri_sample_percentages.append(np.round(ri_indices.shape[0] / y_test.shape[0] * 100, 1))\n",
    "    \n",
    "#     # calculate the accuracy for these indices\n",
    "#     accuracy = accuracy_score(ri_actual, ri_predictions)\n",
    "#     accuracies.append(accuracy)\n",
    "    \n",
    "# accuracies = np.round(np.array(accuracies), 2)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(0, 11), accuracies, marker = 'o')\n",
    "plt.title('Accuracy vs Reliability Index')\n",
    "plt.xlabel('Reliability Index')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# add another x-axis for the percentage of samples in each reliability index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
